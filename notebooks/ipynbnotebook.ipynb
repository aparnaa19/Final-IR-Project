{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3f9bc4",
   "metadata": {},
   "source": [
    "# Information Retrieval Search Engine\n",
    "\n",
    "**Student:** Aparnaa Mahalaxmi Arulljothi A20560995 \n",
    "\n",
    "---\n",
    "\n",
    "1. **Crawler** – Scrapy tool used to gather a small set of HTML pages  \n",
    "2. **Indexer** – Builds a TF-IDF document index with scikit-learn  \n",
    "3. **Query Engine** – Ranks documents using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9474718",
   "metadata": {},
   "source": [
    "# Information Retrieval Search Engine Project\n",
    "\n",
    "- Scrapy-based Wikipedia crawl and TF-IDF index\n",
    "- TF–IDF search over the given three HTML documents and a separately indexed Wikipedia corpus\n",
    "- Query processing pipeline that writes ranked results to `results.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8f3d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ready: data\\wiki_corpus\n",
      "Directory ready: data\\html_corpus\n",
      "Directory ready: data\\output\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BASE_DIRS = [\n",
    "    Path(\"data/wiki_corpus\"),   # crawled Wikipedia pages\n",
    "    Path(\"data/html_corpus\"),   # given 3 HTML files\n",
    "    Path(\"data/output\"),        # index.json, wikipedia_index.json, results.csv\n",
    "]\n",
    "for path in BASE_DIRS:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory ready: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01061c5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Web Crawler\n",
    "\n",
    "Scrapy spider used to collect a small Wikipedia corpus.\n",
    "\n",
    "**Config:**\n",
    "- Start URL: https://en.wikipedia.org/wiki/Information_retrieval  \n",
    "- Depth: up to 2 link levels  \n",
    "- Max pages: 100  \n",
    "- Saved as HTML under `data/wiki_corpus/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f058729",
   "metadata": {},
   "source": [
    "### HTML text extraction helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_html(path: Path) -> str:\n",
    "    \"\"\"Return plain text extracted from an HTML file.\"\"\"\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            html = f.read()\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\", \"meta\", \"header\", \"footer\"]):\n",
    "            tag.decompose()\n",
    "        for div in soup.find_all(\"div\", {\"class\": [\"mw-navigation\", \"vector-menu-content\"]}):  \n",
    "            div.decompose()\n",
    "        text = soup.get_text(\" \", strip=True)  # Extract clean text        \n",
    "        text = re.sub(r\"\\s+\", \" \", text) # Remove multiple spaces\n",
    "        return text\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to read {path}: {exc}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80f33d",
   "metadata": {},
   "source": [
    "---\n",
    "##  Scrapy spider to grab up to 100 Wikipedia pages on Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747e3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from pathlib import Path\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikipediaIR(scrapy.Spider):\n",
    "    name = \"wikipedia_ir\"\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Information_retrieval\"]\n",
    "\n",
    "    # Parameters\n",
    "    custom_settings = {\n",
    "        \"DEPTH_LIMIT\": 2,                \n",
    "        \"CLOSESPIDER_PAGECOUNT\": 100,     \n",
    "        \"ROBOTSTXT_OBEY\": True,\n",
    "        \"DOWNLOAD_DELAY\": 1.0,\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "        \"AUTOTHROTTLE_START_DELAY\": 1.0,\n",
    "        \"AUTOTHROTTLE_MAX_DELAY\": 5.0,\n",
    "        \"AUTOTHROTTLE_TARGET_CONCURRENCY\": 1.0,\n",
    "        \"LOG_LEVEL\": \"INFO\",\n",
    "        \"USER_AGENT\": \"IRCourseCrawler/1.0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_dir = Path(\"data/wiki_corpus\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.page_counter = 0\n",
    "\n",
    "    def parse(self, response):           # Extract the Wikipedia page title from the URL\n",
    "        title = response.url.split(\"/wiki/\")[-1]\n",
    "        title = title.replace(\" \", \"_\")        # Normalize any spaces\n",
    "        filename = f\"{title}.html\"\n",
    "        filepath = self.output_dir / filename\n",
    "        filepath.write_bytes(response.body)\n",
    "        self.logger.info(f\"Saved {filepath} (Depth: {response.meta.get('depth')})\") \n",
    "        self.page_counter += 1\n",
    "        if self.page_counter >= 100:                     # Stop if we have reached the max page limit\n",
    "            self.logger.info(\"Reached 100 pages. Stopping crawl.\")\n",
    "            return\n",
    "                 \n",
    "        for href in response.css(\"a::attr(href)\").getall():  # Follows internal Wikipedia article\n",
    "            if href.startswith(\"/wiki/\") and not href.startswith(\"/wiki/Special:\") and not href.startswith(\"#\"):\n",
    "                yield response.follow(href, callback=self.parse)\n",
    "\n",
    "def run_crawler():\n",
    "    print(\"Wikipedia Crawl Configuration\")\n",
    "    print(\"Max Depth     : 2\")\n",
    "    print(\"Page Limit    : 100\\n\")\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(WikipediaIR)\n",
    "    process.start()\n",
    "    print(\"\\nCrawl completed.\\n\")\n",
    "\n",
    "\n",
    "#run_crawler()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4248f98",
   "metadata": {},
   "source": [
    "---\n",
    "### Preview of crawled Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db27c4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth     : 1\n",
      "Page Limit    : 10\n",
      "\n",
      "Total HTML files found: 100\n",
      "Using 100 pages for indexing\n",
      "\n",
      "First 20 files:\n",
      "  1966_flood_of_the_Arno.html\n",
      "  3D_retrieval.html\n",
      "  BERT_(language_model).html\n",
      "  Conservation-restoration_of_Leonardo_da_Vinci%27s_The_Last_Supper.html\n",
      "  Conservation-restoration_of_the_H.L._Hunley.html\n",
      "  Conservation-restoration_of_the_Shroud_of_Turin.html\n",
      "  Conservation-restoration_of_the_Statue_of_Liberty.html\n",
      "  Conservation-restoration_of_Thomas_Eakins%27_The_Gross_Clinic.html\n",
      "  Conservation_and_restoration_of_Pompeian_frescoes.html\n",
      "  Conservation_and_restoration_of_rail_vehicles.html\n",
      "  Conservation_issues_of_Pompeii_and_Herculaneum.html\n",
      "  Desktop_search.html\n",
      "  Digital_libraries.html\n",
      "  Ecce_Homo_(Garc%C3%ADa_Mart%C3%ADnez_and_Gim%C3%A9nez).html\n",
      "  ElgooG.html\n",
      "  Enterprise_search.html\n",
      "  Ethnochoreology.html\n",
      "  Ethnopoetics.html\n",
      "  Family_folklore.html\n",
      "  Federated_search.html\n",
      "\n",
      "Preview of: 1966_flood_of_the_Arno.html\n",
      "\n",
      "Extracted text (first 500 chars):\n",
      "1966 flood of the Arno - Wikipedia Jump to content Contents move to sidebar hide (Top) 1 Overview 2 Timeline of events Toggle Timeline of events subsection 2.1 3 November 2.2 4 November 3 Impact Toggle Impact subsection 3.1 Collections affected 3.2 Works affected 4 Funding and assistance Toggle Funding and assistance subsection 4.1 The \"Mud Angels\" 4.2 The \"Flood Ladies\" 5 Conservation measures Toggle Conservation measures subsection 5.1 Books and records 5.1.1 The National Library Centers of Fl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"Max Depth     : 1\")\n",
    "print(\"Page Limit    : 10\\n\")\n",
    "\n",
    "wiki_dir = Path(\"data/wiki_corpus\")\n",
    "html_files = sorted(wiki_dir.glob(\"*.html\"))\n",
    "\n",
    "print(f\"Total HTML files found: {len(html_files)}\")\n",
    "\n",
    "# cap at 100 pages \n",
    "wiki_subset = html_files[:100]\n",
    "print(f\"Using {len(wiki_subset)} pages for indexing\\n\")\n",
    "\n",
    "print(\"First 20 files:\")\n",
    "for f in wiki_subset[:20]:\n",
    "    print(\" \", f.name)\n",
    "\n",
    "# a preview of the first crawled page\n",
    "if wiki_subset:\n",
    "    first_page = wiki_subset[0]\n",
    "    print(f\"\\nPreview of: {first_page.name}\")\n",
    "    text = read_clean_html(first_page)\n",
    "\n",
    "    print(\"\\nExtracted text (first 500 chars):\")\n",
    "    print(text[:500])\n",
    "else:\n",
    "    print(\"No pages found in data/wiki_corpus/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab385a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Load and inspect cleaned Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a8e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading cleaned Wikipedia pages\n",
      "Total documents loaded: 100\n",
      "Length: 31664 chars\n",
      "Preview: 1966 flood of the Arno - Wikipedia Jump to content Contents move to sidebar hide (Top) 1 Overview 2 Timeline of events Toggle Timeline of events subsection 2.1 3 November 2.2 4 November 3 Impact Toggl\n",
      "Length: 7135 chars\n",
      "Preview: 3D Content Retrieval - Wikipedia Jump to content Contents move to sidebar hide (Top) 1 3D retrieval methods 2 3D Engineering Search System 3 Challenges 4 See also 5 References English Tools Tools move\n",
      "Length: 46181 chars\n",
      "Preview: BERT (language model) - Wikipedia Jump to content Contents move to sidebar hide (Top) 1 Architecture Toggle Architecture subsection 1.1 Embedding 1.2 Architectural family 2 Training Toggle Training su\n"
     ]
    }
   ],
   "source": [
    "# Build a small doc dictionary from the crawled pages\n",
    "wiki_docs = {}\n",
    "\n",
    "print(\"\\nLoading cleaned Wikipedia pages\")\n",
    "for html_file in wiki_subset:\n",
    "    doc_id = html_file.stem       \n",
    "    text = read_clean_html(html_file)\n",
    "    wiki_docs[doc_id] = text\n",
    "\n",
    "print(f\"Total documents loaded: {len(wiki_docs)}\")\n",
    "for i, (doc_id, text) in enumerate(wiki_docs.items()):  # few samples\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(f\"Length: {len(text)} chars\")\n",
    "    print(f\"Preview: {text[:200]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64f80b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Build TF–IDF index for Wikipedia corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0117652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building TF-IDF index for Wikipedia corpus\n",
      "Documents: 100\n",
      "TF–IDF matrix created\n",
      "Documents: 100\n",
      "Vocabulary size: 35641 terms\n",
      "Matrix shape: (100, 35641)\n",
      "Location: data\\output\\wikipedia_index.json\n",
      "Index summary:\n",
      "100 documents\n",
      "35641 terms\n"
     ]
    }
   ],
   "source": [
    "# TF–IDF index for the crawled Wikipedia pages\n",
    "print(\"\\nBuilding TF-IDF index for Wikipedia corpus\")\n",
    "\n",
    "wiki_doc_ids = list(wiki_docs.keys())\n",
    "wiki_texts = [wiki_docs[d] for d in wiki_doc_ids]\n",
    "\n",
    "print(f\"Documents: {len(wiki_doc_ids)}\")\n",
    "\n",
    "vectorizer_wiki = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    norm=\"l2\",\n",
    ")\n",
    "\n",
    "wiki_tfidf = vectorizer_wiki.fit_transform(wiki_texts)\n",
    "terms_wiki = vectorizer_wiki.get_feature_names_out()\n",
    "\n",
    "print(\"TF–IDF matrix created\")\n",
    "print(f\"Documents: {wiki_tfidf.shape[0]}\")\n",
    "print(f\"Vocabulary size: {wiki_tfidf.shape[1]} terms\")\n",
    "print(f\"Matrix shape: {wiki_tfidf.shape}\")\n",
    "\n",
    "sparsity = 1 - wiki_tfidf.nnz / (wiki_tfidf.shape[0] * wiki_tfidf.shape[1])\n",
    "wikipedia_index = {\n",
    "    \"document_ids\": wiki_doc_ids,\n",
    "    \"vocabulary\": terms_wiki.tolist(),\n",
    "    \"tfidf_matrix\": wiki_tfidf.toarray().tolist(),\n",
    "    \"vectorizer_params\": {\n",
    "        \"lowercase\": True,\n",
    "        \"stop_words\": \"english\",\n",
    "        \"norm\": \"l2\",\n",
    "    },\n",
    "}\n",
    "\n",
    "wikipedia_index_path = Path(\"data/output/wikipedia_index.json\")\n",
    "wikipedia_index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with wikipedia_index_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(wikipedia_index, f, indent=2)\n",
    "print(f\"Location: {wikipedia_index_path}\")\n",
    "print(\"Index summary:\")\n",
    "print(f\"{len(wikipedia_index['document_ids'])} documents\")\n",
    "print(f\"{len(wikipedia_index['vocabulary'])} terms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89716b2b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Inspect saved Wikipedia TF–IDF index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddcbb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia TFIDF index from: data\\output\\wikipedia_index.json\n",
      "\n",
      "Index summary\n",
      "Documents: 100\n",
      "Vocabulary size: 35641\n",
      "TFIDF matrix: 100 x 35641\n",
      "\n",
      "Sample document IDs:\n",
      "  1966_flood_of_the_Arno\n",
      "  3D_retrieval\n",
      "  BERT_(language_model)\n",
      "  Conservation-restoration_of_Leonardo_da_Vinci%27s_The_Last_Supper\n",
      "  Conservation-restoration_of_the_H.L._Hunley\n",
      "\n",
      "Sample vocabulary terms:\n",
      "  00\n",
      "  000\n",
      "  0000\n",
      "  00008\n",
      "  0001\n",
      "  0002\n",
      "  00022\n",
      "  000271620258300113\n",
      "  00036\n",
      "  0004\n",
      "  00055\n",
      "  00059\n",
      "  0006\n",
      "  0009\n",
      "  000s\n",
      "\n",
      "First document vector length: 35641\n",
      "First 20 TFIDF values: [0.038543787214482, 0.0877668082367568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011105589430370672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Load wikipedia_index.json and print a quick summary\n",
    "index_path = Path(\"data/output/wikipedia_index.json\")\n",
    "print(\"Loading Wikipedia TFIDF index from:\", index_path)\n",
    "\n",
    "with index_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    wikipedia_index = json.load(f)\n",
    "\n",
    "print(\"\\nIndex summary\")\n",
    "print(\"Documents:\", len(wikipedia_index[\"document_ids\"]))\n",
    "print(\"Vocabulary size:\", len(wikipedia_index[\"vocabulary\"]))\n",
    "print(\n",
    "    \"TFIDF matrix:\",\n",
    "    len(wikipedia_index[\"tfidf_matrix\"]),\n",
    "    \"x\",\n",
    "    len(wikipedia_index[\"tfidf_matrix\"][0]),\n",
    ")\n",
    "\n",
    "# a few document IDs\n",
    "print(\"\\nSample document IDs:\")\n",
    "for doc_id in wikipedia_index[\"document_ids\"][:5]:\n",
    "    print(\" \", doc_id)\n",
    "\n",
    "# a few vocabulary terms\n",
    "print(\"\\nSample vocabulary terms:\")\n",
    "for term in wikipedia_index[\"vocabulary\"][:15]:\n",
    "    print(\" \", term)\n",
    "\n",
    "# first document vector preview\n",
    "first_vec = wikipedia_index[\"tfidf_matrix\"][0]\n",
    "print(\"\\nFirst document vector length:\", len(first_vec))\n",
    "print(\"First 20 TFIDF values:\", first_vec[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c08b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Wikipedia TFIDF index\n",
      "Loaded index with 100 documents and 35641 terms.\n",
      "\n",
      "Query: \"information retrieval system\"\n",
      "\n",
      "Top matching Wikipedia documents:\n",
      "\n",
      "1. Information_retrieval                         similarity = 0.714\n",
      "2. Information_filtering                         similarity = 0.365\n",
      "3. Music_information_retrieval                   similarity = 0.314\n",
      "4. Image_retrieval                               similarity = 0.278\n",
      "5. 3D_retrieval                                  similarity = 0.262\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nLoading Wikipedia TFIDF index\")\n",
    "index_path = Path(\"data/output/wikipedia_index.json\")\n",
    "with index_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    wiki_index = json.load(f)\n",
    "\n",
    "doc_ids = wiki_index[\"document_ids\"]\n",
    "vocab = wiki_index[\"vocabulary\"]\n",
    "tfidf_matrix = np.array(wiki_index[\"tfidf_matrix\"])\n",
    "print(f\"Loaded index with {len(doc_ids)} documents and {len(vocab)} terms.\\n\")\n",
    "vocab_index = {term: i for i, term in enumerate(vocab)}\n",
    "def vectorize_query(query: str) -> np.ndarray: # Convert a query into a vector\n",
    "    q_vec = np.zeros(len(vocab))\n",
    "    for token in query.lower().split():\n",
    "        if token in vocab_index:\n",
    "            q_vec[vocab_index[token]] += 1.0   # simple weighted count\n",
    "    return q_vec\n",
    "def cosine_similarity(matrix, vector):\n",
    "    vec_norm = np.linalg.norm(vector)\n",
    "    doc_norms = np.linalg.norm(matrix, axis=1)\n",
    "    sims = np.zeros(matrix.shape[0])\n",
    "    valid = (vec_norm != 0) & (doc_norms != 0)\n",
    "    sims[valid] = (matrix[valid] @ vector) / (doc_norms[valid] * vec_norm)\n",
    "    return sims\n",
    "query = \"information retrieval system\"\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "q_vec = vectorize_query(query)\n",
    "scores = cosine_similarity(tfidf_matrix, q_vec)\n",
    "top_k = 5  # Return top-5 results\n",
    "ranked = scores.argsort()[::-1][:top_k]\n",
    "print(\"Top matching Wikipedia documents:\\n\")\n",
    "for rank, idx in enumerate(ranked, start=1):\n",
    "    print(f\"{rank}. {doc_ids[idx]:45s} similarity = {scores[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649a77f",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Indexer\n",
    "\n",
    "Input: 3 HTML files from `data/html_corpus/`  \n",
    "  - `0F64A61C-DF01-4F43-8B8D-F0319C41768E.html`\n",
    "  - `1F648A7F-2C64-458C-BFAF-463A071530ED.html`\n",
    "  - `6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html`\n",
    "Pipeline: HTML → cleaned text → TF–IDF → document–term matrix → `index.json`  \n",
    "Output: `index.json` with document IDs, vocabulary, and TF–IDF weights for query processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f7ccc",
   "metadata": {},
   "source": [
    "### HTML parsing and clean text extraction (3 docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fd994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking official files...\n",
      "\n",
      "Loaded: 0F64A61C-DF01-4F43-8B8D-F0319C41768E.html\n",
      "Text length: 56849 characters\n",
      "\n",
      "Loaded: 1F648A7F-2C64-458C-BFAF-463A071530ED.html\n",
      "Text length: 78758 characters\n",
      "\n",
      "Loaded: 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html\n",
      "Text length: 37277 characters\n",
      "\n",
      "Total documents loaded: 3\n"
     ]
    }
   ],
   "source": [
    "# Load 3 given  HTML documents and clean their text\n",
    "official_files = [\n",
    "    \"0F64A61C-DF01-4F43-8B8D-F0319C41768E.html\",\n",
    "    \"1F648A7F-2C64-458C-BFAF-463A071530ED.html\",\n",
    "    \"6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html\",\n",
    "]\n",
    "\n",
    "corpus_dir = Path(\"data/html_corpus\")\n",
    "print(\"Checking official files...\")\n",
    "\n",
    "docs = {}\n",
    "for fname in official_files:\n",
    "    file_path = corpus_dir / fname\n",
    "    if file_path.exists():\n",
    "        doc_id = fname.replace(\".html\", \"\")\n",
    "        text = read_clean_html(file_path)\n",
    "        docs[doc_id] = text\n",
    "        print(f\"\\nLoaded: {fname}\")\n",
    "        print(f\"Text length: {len(text)} characters\")\n",
    "    else:\n",
    "        print(f\"\\nMissing: {fname}\")\n",
    "print(f\"\\nTotal documents loaded: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c06fc",
   "metadata": {},
   "source": [
    "---\n",
    "### Build TF–IDF vectors for the 3 given documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ec1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TFIDF index\n",
      "TFIDF matrix created\n",
      "Documents: 3\n",
      "Vocabulary size: 4544 unique terms\n",
      "Matrix shape: (3, 4544)\n"
     ]
    }
   ],
   "source": [
    "# TF–IDF matrix for the three given docs\n",
    "print(\"Building TFIDF index\")\n",
    "\n",
    "doc_ids = list(docs.keys())\n",
    "doc_texts = [docs[d] for d in doc_ids]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    norm=\"l2\",\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(doc_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"TFIDF matrix created\")\n",
    "print(f\"Documents: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"Vocabulary size: {tfidf_matrix.shape[1]} unique terms\")\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "sparsity = 1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299c459",
   "metadata": {},
   "source": [
    "---\n",
    "### Save TF–IDF index as index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a387a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved successfully\n",
      "Location: data\\output\\index.json\n",
      "File size: 333.92 KB\n",
      "\n",
      "Index Has:\n",
      "3 documents\n",
      "4544 vocabulary terms\n",
      "TFIDF matrix: 3 x 4544\n"
     ]
    }
   ],
   "source": [
    "# Create the JSON index\n",
    "index_data = {\n",
    "    \"document_ids\": doc_ids,\n",
    "    \"vocabulary\": feature_names.tolist(),\n",
    "    \"tfidf_matrix\": tfidf_matrix.toarray().tolist(),\n",
    "    \"vectorizer_params\": {\n",
    "        \"lowercase\": True,\n",
    "        \"stop_words\": \"english\",\n",
    "        \"norm\": \"l2\",\n",
    "    },\n",
    "}\n",
    "\n",
    "index_path = Path(\"data/output/index.json\")\n",
    "index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with index_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(index_data, f, indent=2)\n",
    "\n",
    "print(\"Index saved successfully\")\n",
    "print(\"Location:\", index_path)\n",
    "print(f\"File size: {os.path.getsize(index_path) / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nIndex Has:\")\n",
    "print(len(index_data[\"document_ids\"]), \"documents\")\n",
    "print( len(index_data[\"vocabulary\"]), \"vocabulary terms\")\n",
    "print(\n",
    "    \"TFIDF matrix:\",\n",
    "    len(index_data[\"tfidf_matrix\"]),\n",
    "    \"x\",\n",
    "    len(index_data[\"tfidf_matrix\"][0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d28d11",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Query Processor\n",
    "\n",
    "Input: `index.json` (TF–IDF index) and `queries.csv` (query_id, query_text)  \n",
    "Pipeline: queries → TF–IDF → cosine similarity → ranked list → `results.csv`  \n",
    "Output: `results.csv` with columns: query_id, rank, document_id (all 3 docs ranked per query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917044b7",
   "metadata": {},
   "source": [
    "### Load index.json and queries.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from: data\\output\\index.json\n",
      "\n",
      "Index loaded\n",
      "Docs: 3\n",
      "Vocab size: 4544\n",
      "Matrix: 3 x 4544\n",
      "\n",
      "Queries loaded: 3\n",
      "\n",
      "Query preview:\n",
      "  6E93CDD1-52F9-4F41-A405-54E398EF6FF8: \"information overload\"\n",
      "  0D97BCC6-C46E-4242-9777-7CEAED55B362: \"database server hardware specs\"\n",
      "  78452FF4-94D7-422C-9283-A14615C44ADC: \"search engine open sorce\"\n"
     ]
    }
   ],
   "source": [
    "# Load TF–IDF index and queries\n",
    "index_path = Path(\"data/output/index.json\")\n",
    "print(\"Loading index from:\", index_path)\n",
    "\n",
    "with index_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    idx = json.load(f)\n",
    "\n",
    "doc_ids_loaded = idx[\"document_ids\"]\n",
    "vocab_loaded = idx[\"vocabulary\"]\n",
    "tfidf_loaded = np.array(idx[\"tfidf_matrix\"])\n",
    "\n",
    "print(\"\\nIndex loaded\")\n",
    "print(f\"Docs: {len(doc_ids_loaded)}\")\n",
    "print(f\"Vocab size: {len(vocab_loaded)}\")\n",
    "print(f\"Matrix: {tfidf_loaded.shape[0]} x {tfidf_loaded.shape[1]}\")\n",
    "\n",
    "# Load queries\n",
    "queries = []\n",
    "with open(\"queries.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        queries.append({\n",
    "            \"query_id\": row[\"query_id\"],\n",
    "            \"query_text\": row[\"query_text\"],\n",
    "        })\n",
    "\n",
    "print(\"\\nQueries loaded:\", len(queries))\n",
    "\n",
    "# preview\n",
    "print(\"\\nQuery preview:\")\n",
    "for q in queries:\n",
    "    print(f\"  {q['query_id']}: \\\"{q['query_text']}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5feba",
   "metadata": {},
   "source": [
    "---\n",
    "### Query processing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998eefe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query processor\n"
     ]
    }
   ],
   "source": [
    "def process_query(query_text, vocabulary, tfidf_matrix, doc_ids):\n",
    "    \"\"\"Run a query and return ranked docs.\"\"\" \n",
    "    # vectorize using the same vocab\n",
    "    vec_q = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        vocabulary=vocabulary,\n",
    "        norm=\"l2\",\n",
    "    )\n",
    "    q_vec = vec_q.fit_transform([query_text]).toarray()\n",
    "    sims = sklearn_cosine(q_vec, tfidf_matrix)[0]   \n",
    "    pairs = list(zip(doc_ids, sims))  # pair (doc_id, score) and sort\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    ranked = [(i + 1, doc, score) for i, (doc, score) in enumerate(pairs)]  # add rank (1-based)\n",
    "    return ranked\n",
    "\n",
    "print(\"Query processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90edb4c",
   "metadata": {},
   "source": [
    "---\n",
    "### Sample query analysis (query 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ccd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed query analysis\n",
      "\n",
      "Query ID:   6E93CDD1-52F9-4F41-A405-54E398EF6FF8\n",
      "Query text: information overload\n",
      "Query terms: ['information', 'overload']\n",
      "\n",
      "Ranked documents:\n",
      "  1: 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3   score=0.361912\n",
      "  2: 0F64A61C-DF01-4F43-8B8D-F0319C41768E   score=0.072527\n",
      "  3: 1F648A7F-2C64-458C-BFAF-463A071530ED   score=0.067744\n",
      "\n",
      "Query terms found in vocabulary: ['information', 'overload']\n"
     ]
    }
   ],
   "source": [
    "#  first query \n",
    "print(\"Detailed query analysis\\n\")\n",
    "sample = queries[0]\n",
    "qid = sample[\"query_id\"]\n",
    "qtext = sample[\"query_text\"]\n",
    "print(\"Query ID:  \", qid)\n",
    "print(\"Query text:\", qtext)\n",
    "print(\"Query terms:\", qtext.lower().split())\n",
    "# ranking\n",
    "ranked = process_query(\n",
    "    qtext,\n",
    "    vocab_loaded,\n",
    "    tfidf_loaded,\n",
    "    doc_ids_loaded,\n",
    ")\n",
    "print(\"\\nRanked documents:\")\n",
    "for rank, doc_id, score in ranked:\n",
    "    print(f\"  {rank}: {doc_id}   score={score:.6f}\")\n",
    "present_terms = [t for t in qtext.lower().split() if t in vocab_loaded] # which query terms are in the vocab\n",
    "print(\"\\nQuery terms found in vocabulary:\", present_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac122501",
   "metadata": {},
   "source": [
    "---\n",
    "### Run all queries and save ranked results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac18235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running queries\n",
      "\n",
      "Query 6E93CDD1-52F9-4F41-A405-54E398EF6FF8: \"information overload\"\n",
      "  1. 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3  (0.3619)\n",
      "  2. 0F64A61C-DF01-4F43-8B8D-F0319C41768E  (0.0725)\n",
      "  3. 1F648A7F-2C64-458C-BFAF-463A071530ED  (0.0677)\n",
      "\n",
      "Query 0D97BCC6-C46E-4242-9777-7CEAED55B362: \"database server hardware specs\"\n",
      "  1. 1F648A7F-2C64-458C-BFAF-463A071530ED  (0.3691)\n",
      "  2. 0F64A61C-DF01-4F43-8B8D-F0319C41768E  (0.0227)\n",
      "  3. 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3  (0.0158)\n",
      "\n",
      "Query 78452FF4-94D7-422C-9283-A14615C44ADC: \"search engine open sorce\"\n",
      "  1. 0F64A61C-DF01-4F43-8B8D-F0319C41768E  (0.5569)\n",
      "  2. 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3  (0.1221)\n",
      "  3. 1F648A7F-2C64-458C-BFAF-463A071530ED  (0.0287)\n",
      "\n",
      "Saved results to: data\\output\\results.csv\n",
      "Total rows: 9\n"
     ]
    }
   ],
   "source": [
    "# process each query and collect all rankings\n",
    "print(\"Running queries\")\n",
    "all_results = []\n",
    "for q in queries:\n",
    "    qid = q[\"query_id\"]\n",
    "    qtext = q[\"query_text\"]\n",
    "    print(f\"\\nQuery {qid}: \\\"{qtext}\\\"\")\n",
    "    ranked = process_query(\n",
    "        qtext,\n",
    "        vocab_loaded,\n",
    "        tfidf_loaded,\n",
    "        doc_ids_loaded,\n",
    "    )\n",
    "    for rank, doc_id, score in ranked:\n",
    "        print(f\"  {rank}. {doc_id}  ({score:.4f})\")\n",
    "        all_results.append({\n",
    "            \"query_id\": qid,\n",
    "            \"rank\": rank,\n",
    "            \"document_id\": doc_id,\n",
    "        })\n",
    "out_path = Path(\"data/output/results.csv\") # save results.csv\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"query_id\", \"rank\", \"document_id\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_results)\n",
    "print(\"\\nSaved results to:\", out_path)\n",
    "print(\"Total rows:\", len(all_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba2be3",
   "metadata": {},
   "source": [
    "---\n",
    "### Preview of generated results.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21ffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from: data\\output\\results.csv\n",
      "\n",
      "First few rows:\n",
      "  6E93CDD1-52F9-4F41-A405-54E398EF6FF8  rank=1  doc=6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3\n",
      "  6E93CDD1-52F9-4F41-A405-54E398EF6FF8  rank=2  doc=0F64A61C-DF01-4F43-8B8D-F0319C41768E\n",
      "  6E93CDD1-52F9-4F41-A405-54E398EF6FF8  rank=3  doc=1F648A7F-2C64-458C-BFAF-463A071530ED\n",
      "  0D97BCC6-C46E-4242-9777-7CEAED55B362  rank=1  doc=1F648A7F-2C64-458C-BFAF-463A071530ED\n",
      "  0D97BCC6-C46E-4242-9777-7CEAED55B362  rank=2  doc=0F64A61C-DF01-4F43-8B8D-F0319C41768E\n",
      "  0D97BCC6-C46E-4242-9777-7CEAED55B362  rank=3  doc=6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3\n",
      "  78452FF4-94D7-422C-9283-A14615C44ADC  rank=1  doc=0F64A61C-DF01-4F43-8B8D-F0319C41768E\n",
      "  78452FF4-94D7-422C-9283-A14615C44ADC  rank=2  doc=6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3\n",
      "  78452FF4-94D7-422C-9283-A14615C44ADC  rank=3  doc=1F648A7F-2C64-458C-BFAF-463A071530ED\n",
      "\n",
      "Total rows: 9\n"
     ]
    }
   ],
   "source": [
    "# Read results.csv \n",
    "results_path = Path(\"data/output/results.csv\")\n",
    "print(\"Loading results from:\", results_path)\n",
    "\n",
    "with results_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader)\n",
    "print(\"\\nFirst few rows:\")\n",
    "for row in rows[:10]:\n",
    "    print(f\"  {row['query_id']}  rank={row['rank']}  doc={row['document_id']}\")\n",
    "\n",
    "print(\"\\nTotal rows:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06f9ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional Enhancements\n",
    "\n",
    "1. **Spelling Correction (NLTK)**\n",
    "   - Detects and corrects misspelled query terms, It uses Uses dictionary-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6e3ab",
   "metadata": {},
   "source": [
    "### Optional: simple spell checker with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English word list loaded: 235892\n",
      "\n",
      "Spelling correction examples:\n",
      "  infomation   infumation    (corrected)\n",
      "  Computr      computer      (corrected)\n",
      "  seaach       search        (corrected)\n",
      "  retrieval    retrieval     (unchanged)\n"
     ]
    }
   ],
   "source": [
    "# NLTK-based spelling helper (for noisy queries)\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "nltk.download(\"words\", quiet=True)\n",
    "\n",
    "english_vocab = set(words.words())\n",
    "print(\"English word list loaded:\", len(english_vocab))\n",
    "\n",
    "def correct_spelling(word: str) -> str:\n",
    "    \"\"\"Return a simple spelling correction (or the word itself).\"\"\"\n",
    "    w = word.lower()\n",
    "   \n",
    "    if w in english_vocab:  # already a known word\n",
    "        return w\n",
    "\n",
    "    # small pool of candidates with similar length\n",
    "    cand = [t for t in english_vocab if abs(len(t) - len(w)) <= 2]\n",
    "    if not cand:\n",
    "        return w\n",
    "\n",
    "    best = min(cand, key=lambda t: edit_distance(w, t))\n",
    "\n",
    "    # only accept close matches\n",
    "    if edit_distance(w, best) <= 2:\n",
    "        return best\n",
    "    return w\n",
    "\n",
    "print(\"\\nSpelling correction examples:\")\n",
    "for w in [\"infomation\", \"Computr\", \"seaach\", \"retrieval\"]:\n",
    "    fixed = correct_spelling(w)\n",
    "    status = \"Changed\" if fixed != w else \"unchanged\"\n",
    "    print(f\"  {w:12} {fixed:12}  ({status})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36b157",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Flask REST API\n",
    "\n",
    "Simple REST interface for running searches.\n",
    "\n",
    "**Request:** `POST /search` with `{\"query\": \"text\", \"top_k\": 3}`  \n",
    "**Response:** ranked documents and scores  \n",
    "**Run:** `python flask_app.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97da68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask API \n"
     ]
    }
   ],
   "source": [
    "# Flask API \n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# globals for the loaded index\n",
    "vocab_api = None\n",
    "matrix_api = None\n",
    "docs_api = None\n",
    "\n",
    "\n",
    "@app.route(\"/search\", methods=[\"POST\"])\n",
    "def api_search():\n",
    "    \"\"\"Handle search requests and return ranked results.\"\"\"\n",
    "    try:\n",
    "        body = request.get_json(silent=True)\n",
    "        if not body:\n",
    "            return jsonify({\"error\": \"Missing JSON payload\"}), 400\n",
    "\n",
    "        query = body.get(\"query\")\n",
    "        if not query:\n",
    "            return jsonify({\"error\": \"Field 'query' is required\"}), 400\n",
    "\n",
    "        top_k = int(body.get(\"top_k\", 3))\n",
    "\n",
    "        if vocab_api is None or matrix_api is None or docs_api is None:\n",
    "            return jsonify({\"error\": \"Index not initialized\"}), 503\n",
    "\n",
    "        results = process_query(query, vocab_api, matrix_api, docs_api)\n",
    "\n",
    "        output = [\n",
    "            {\n",
    "                \"rank\": r,\n",
    "                \"document_id\": d,\n",
    "                \"score\": float(s),\n",
    "            }\n",
    "            for r, d, s in results[:top_k]\n",
    "        ]\n",
    "\n",
    "        return jsonify({\n",
    "            \"query\": query,\n",
    "            \"count\": len(output),\n",
    "            \"results\": output,\n",
    "        }), 200\n",
    "\n",
    "    except Exception as err:\n",
    "        return jsonify({\"error\": str(err)}), 500\n",
    "\n",
    "\n",
    "@app.route(\"/healthcheck\", methods=[\"GET\"])\n",
    "def api_health():\n",
    "    \"\"\"Basic health and index status.\"\"\"\n",
    "    count = len(docs_api) if docs_api else 0\n",
    "    return jsonify({\n",
    "        \"status\": \"running\",\n",
    "        \"documents_loaded\": count,\n",
    "    }), 200\n",
    "\n",
    "\n",
    "def load_index():\n",
    "    \"\"\"Load the index.json file into memory.\"\"\"\n",
    "    global vocab_api, matrix_api, docs_api\n",
    "\n",
    "    idx_path = Path(\"data/output/index.json\")\n",
    "    with idx_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    docs_api = data[\"document_ids\"]\n",
    "    vocab_api = data[\"vocabulary\"]\n",
    "    matrix_api = np.array(data[\"tfidf_matrix\"])\n",
    "\n",
    "    print(f\"[API] Loaded {len(docs_api)} docs and {len(vocab_api)} terms.\")\n",
    "\n",
    "\n",
    "# Run the API (use this when running as a script)\n",
    "# if __name__ == \"__main__\":\n",
    "#     load_index()\n",
    "#     app.run(host=\"0.0.0.0\", port=7000, debug=True)\n",
    "\n",
    "print(\"Flask API \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48566c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API index loaded: 3 docs, 4544 terms\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "app = Flask(__name__)\n",
    "# Globals for the loaded index\n",
    "vocab_api = None\n",
    "matrix_api = None\n",
    "docs_api = None\n",
    "\n",
    "@app.route(\"/search\", methods=[\"POST\"])\n",
    "def api_search():\n",
    "    \"\"\"Handle search requests and return ranked results.\"\"\"\n",
    "    try:\n",
    "        body = request.get_json(silent=True)\n",
    "        if not body:\n",
    "            return jsonify({\"error\": \"Missing JSON payload\"}), 400\n",
    "\n",
    "        query = body.get(\"query\")\n",
    "        if not query:\n",
    "            return jsonify({\"error\": \"Field 'query' is required\"}), 400\n",
    "\n",
    "        top_k = int(body.get(\"top_k\", 3))\n",
    "\n",
    "        if vocab_api is None or matrix_api is None or docs_api is None:\n",
    "            return jsonify({\"error\": \"Index not initialized\"}), 503\n",
    "\n",
    "        results = process_query(query, vocab_api, matrix_api, docs_api)\n",
    "\n",
    "        output = [\n",
    "            {\n",
    "                \"rank\": r,\n",
    "                \"document_id\": d,\n",
    "                \"score\": float(s),\n",
    "            }\n",
    "            for r, d, s in results[:top_k]\n",
    "        ]\n",
    "\n",
    "        return jsonify({\n",
    "            \"query\": query,\n",
    "            \"count\": len(output),\n",
    "            \"results\": output,\n",
    "        }), 200\n",
    "\n",
    "    except Exception as err:\n",
    "        return jsonify({\"error\": str(err)}), 500\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def api_health():\n",
    "    \"\"\"Basic health and index status.\"\"\"\n",
    "    count = len(docs_api) if docs_api else 0\n",
    "    return jsonify({\n",
    "        \"status\": \"running\",\n",
    "        \"documents_loaded\": count,\n",
    "    }), 200\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    \"\"\"Serve the web interface.\"\"\"\n",
    "    # just redirect or show a message\n",
    "    return jsonify({\n",
    "        \"message\": \"Flask API is running!\",\n",
    "        \"endpoints\": {\n",
    "            \"/search\": \"POST - Search documents\",\n",
    "            \"/health\": \"GET - Health check\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "def load_index():\n",
    "    \"\"\"Load the index.json file into memory.\"\"\"\n",
    "    global vocab_api, matrix_api, docs_api\n",
    "\n",
    "    idx_path = Path(\"data/output/index.json\")\n",
    "    with idx_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    docs_api = data[\"document_ids\"]\n",
    "    vocab_api = data[\"vocabulary\"]\n",
    "    matrix_api = np.array(data[\"tfidf_matrix\"])\n",
    "\n",
    "    print(f\" API index loaded: {len(docs_api)} docs, {len(vocab_api)} terms\")\n",
    "\n",
    "def run_flask_in_background():\n",
    "    \"\"\"Run Flask in a background thread.\"\"\"\n",
    "    app.run(host='127.0.0.1', port=5000, debug=False, use_reloader=False)\n",
    "load_index() \n",
    "# Create and start background thread\n",
    "flask_thread = threading.Thread(target=run_flask_in_background, daemon=True)\n",
    "flask_thread.start()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36710b4",
   "metadata": {},
   "source": [
    "---\n",
    "## **Testing the Flask API on Windows (Command Prompt):**\n",
    "curl -X POST http://127.0.0.1:5000/search -H \"Content-Type: application/json\" -d \"{\\\"query\\\": \\\"information retrieval\\\", \\\"top_k\\\": 3}\"\n",
    "\n",
    "**OUTPUT**: {\"count\":3,\"query\":\"information retrieval\",\"results\":[{\"document_id\":\"6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3\",\"rank\":1,\"score\":0.7248218413342915},{\"document_id\":\"0F64A61C-DF01-4F43-8B8D-F0319C41768E\",\"rank\":2,\"score\":0.0873194287706458},{\"document_id\":\"1F648A7F-2C64-458C-BFAF-463A071530ED\",\"rank\":3,\"score\":0.08337342565124171}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f45d6",
   "metadata": {},
   "source": [
    "https://github.com/aparnaa19/Final-IR-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515bb24",
   "metadata": {},
   "source": [
    "---\n",
    "## Project Structure - Modularized version "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c201673",
   "metadata": {},
   "source": [
    "```project/\n",
    "│\n",
    "├── src/                              \n",
    "│   ├── __init__.py\n",
    "│   ├── crawler.py                    # Scrapy web crawler\n",
    "│   ├── indexer.py                    # TF-IDF indexer\n",
    "│   ├── query_processor.py            # Query ranking engine\n",
    "│   └── utils.py                      # HTML parsing utilities\n",
    "│\n",
    "├── api/                              \n",
    "│   ├── __init__.py\n",
    "│   ├── app.py                        # Flask app\n",
    "│   ├── static/                       # Static files\n",
    "│   │   ├── css/\n",
    "│   │   │   └── style.css            \n",
    "│   │   └── js/\n",
    "│   │       └── script.js            # Frontend JS\n",
    "│   └── templates/\n",
    "│       └── index.html               # Frontend HTML\n",
    "│\n",
    "├── data/                             \n",
    "│   ├── wiki_corpus/                 # Crawled Wikipedia pages \n",
    "│   ├── html_corpus/                 # given 3 HTML files \n",
    "│   │   ├── 0F64A61C-DF01-4F43-8B8D-F0319C41768E.html\n",
    "│   │   ├── 1F648A7F-2C64-458C-BFAF-463A071530ED.html\n",
    "│   │   └── 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html\n",
    "│   └── output/                     \n",
    "│       ├── index.json               # TF-IDF index \n",
    "│       ├── results.csv              # Query results \n",
    "│       └── wikipedia_index.json     # Wikipedia index \n",
    "│\n",
    "├── notebooks/                        \n",
    "│   └── ir_system_report.ipynb        # Jupyter notebook\n",
    "│\n",
    "├── queries.csv                       # Input query file\n",
    "├── requirements.txt                \n",
    "├── README.md    \n",
    "│── build_wiki_index.py                     \n",
    "├── run_pipeline.py                   # Main pipeline script\n",
    "└── test_system.py                    # Quick test script\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Word embedding - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"5b381657-1cb5-4215-9bf6-fd901039dd8d","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Word_embedding","wgTitle":"Word embedding","wgCurRevisionId":1323019878,"wgRevisionId":1323019878,"wgArticleId":43561218,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: publisher location","CS1 errors: ISBN date","CS1: long volume value","Articles with short description","Short description matches Wikidata","All articles lacking reliable references","Articles lacking reliable references from May 2024","All articles containing circular references","Language modeling","Artificial neural networks","Natural language processing","Computational linguistics","Semantic relations"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Word_embedding","wgRelevantArticleId":43561218,"wgTempUserName":null,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgConfirmEditCaptchaNeededForGenericEdit":false,"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":0,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":30000,"wgEditSubmitButtonLabelPublish":true,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q18395344","wgCheckUserClientHintsHeadersJsApi":["brands","architecture","bitness","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGESuggestedEditsTaskTypes":{"taskTypes":["copyedit","link-recommendation"],"unavailableTaskTypes":[]},"wgGETopicsMatchModeEnabled":false,"wgGELevelingUpEnabledForUser":false,"wgGEUseMetricsPlatformExtension":true,"wgMetricsPlatformUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]}};
RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.wikimediamessages.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","mediawiki.page.media","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.bootstrap","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","ext.checkUser.clientHints","ext.quicksurveys.init","ext.growthExperiments.SuggestedEditSession","ext.xLab"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediamessages.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.46.0-wmf.4">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Word_embedding_illustration.svg/1200px-Word_embedding_illustration.svg.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="1115">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="Word embedding - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Word_embedding&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Word_embedding">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="auth.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Word_embedding rootpage-Word_embedding skin-vector-2022 action-view">
<div id="mw-aria-live-region" class="mw-aria-live-region" aria-live="polite"></div><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header no-font-mode-scale">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  title="Main menu" >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li>
		</ul>
		
	</div>
</div>

	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li><li id="n-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages"><span>Special pages</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input mw-searchInput" autocomplete="off"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" spellcheck="false" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  title="Change the appearance of the page&#039;s font size, width, and color" >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-sitesupport-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en" class=""><span>Donate</span></a>
</li>
<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:CreateAccount&amp;returnto=Word+embedding" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:UserLogin&amp;returnto=Word+embedding" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out user-links-collapsible-item"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-sitesupport" class="user-links-collapsible-item mw-list-item"><a href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en"><span>Donate</span></a></li><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Word+embedding" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Word+embedding" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Development_and_history_of_the_approach"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Development_and_history_of_the_approach">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>Development and history of the approach</span>
			</div>
		</a>
		
		<ul id="toc-Development_and_history_of_the_approach-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Polysemy_and_homonymy"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Polysemy_and_homonymy">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>Polysemy and homonymy</span>
			</div>
		</a>
		
		<ul id="toc-Polysemy_and_homonymy-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-For_biological_sequences:_BioVectors"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#For_biological_sequences:_BioVectors">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>For biological sequences: BioVectors</span>
			</div>
		</a>
		
		<ul id="toc-For_biological_sequences:_BioVectors-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Game_design"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Game_design">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>Game design</span>
			</div>
		</a>
		
		<ul id="toc-Game_design-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Sentence_embeddings"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Sentence_embeddings">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>Sentence embeddings</span>
			</div>
		</a>
		
		<ul id="toc-Sentence_embeddings-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Software"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Software">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>Software</span>
			</div>
		</a>
		
			<button aria-controls="toc-Software-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Software subsection</span>
			</button>
		
		<ul id="toc-Software-sublist" class="vector-toc-list">
			<li id="toc-Examples_of_application"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Examples_of_application">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.1</span>
					<span>Examples of application</span>
				</div>
			</a>
			
			<ul id="toc-Examples_of_application-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Ethical_implications"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Ethical_implications">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>Ethical implications</span>
			</div>
		</a>
		
		<ul id="toc-Ethical_implications-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">8</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">9</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar no-font-mode-scale">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  title="Table of Contents" >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Word embedding</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 23 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-23" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">23 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%AA%D8%B6%D9%85%D9%8A%D9%86_%D8%A7%D9%84%D9%83%D9%84%D9%85%D8%A7%D8%AA" title="تضمين الكلمات – Arabic" lang="ar" hreflang="ar" data-title="تضمين الكلمات" data-language-autonym="العربية" data-language-local-name="Arabic" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Incrustaci%C3%B3_de_mots" title="Incrustació de mots – Catalan" lang="ca" hreflang="ca" data-title="Incrustació de mots" data-language-autonym="Català" data-language-local-name="Catalan" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-cs mw-list-item"><a href="https://cs.wikipedia.org/wiki/Vno%C5%99en%C3%AD_slov" title="Vnoření slov – Czech" lang="cs" hreflang="cs" data-title="Vnoření slov" data-language-autonym="Čeština" data-language-local-name="Czech" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/Worteinbettung" title="Worteinbettung – German" lang="de" hreflang="de" data-title="Worteinbettung" data-language-autonym="Deutsch" data-language-local-name="German" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Word_embedding" title="Word embedding – Spanish" lang="es" hreflang="es" data-title="Word embedding" data-language-autonym="Español" data-language-local-name="Spanish" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-eu mw-list-item"><a href="https://eu.wikipedia.org/wiki/Hitz-bektoreak" title="Hitz-bektoreak – Basque" lang="eu" hreflang="eu" data-title="Hitz-bektoreak" data-language-autonym="Euskara" data-language-local-name="Basque" class="interlanguage-link-target"><span>Euskara</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D8%AF%DA%AF%D8%B1%D9%86%D9%85%D8%A7%DB%8C%DB%8C_%D9%88%D8%A7%DA%98%D9%87" title="دگرنمایی واژه – Persian" lang="fa" hreflang="fa" data-title="دگرنمایی واژه" data-language-autonym="فارسی" data-language-local-name="Persian" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Word_embedding" title="Word embedding – French" lang="fr" hreflang="fr" data-title="Word embedding" data-language-autonym="Français" data-language-local-name="French" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9" title="워드 임베딩 – Korean" lang="ko" hreflang="ko" data-title="워드 임베딩" data-language-autonym="한국어" data-language-local-name="Korean" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Word_embedding" title="Word embedding – Italian" lang="it" hreflang="it" data-title="Word embedding" data-language-autonym="Italiano" data-language-local-name="Italian" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/%D7%A9%D7%99%D7%9B%D7%95%D7%9F_%D7%9E%D7%99%D7%9C%D7%99%D7%9D" title="שיכון מילים – Hebrew" lang="he" hreflang="he" data-title="שיכון מילים" data-language-autonym="עברית" data-language-local-name="Hebrew" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E5%8D%98%E8%AA%9E%E3%81%AE%E5%9F%8B%E3%82%81%E8%BE%BC%E3%81%BF" title="単語の埋め込み – Japanese" lang="ja" hreflang="ja" data-title="単語の埋め込み" data-language-autonym="日本語" data-language-local-name="Japanese" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-no mw-list-item"><a href="https://no.wikipedia.org/wiki/Ordembedding" title="Ordembedding – Norwegian Bokmål" lang="nb" hreflang="nb" data-title="Ordembedding" data-language-autonym="Norsk bokmål" data-language-local-name="Norwegian Bokmål" class="interlanguage-link-target"><span>Norsk bokmål</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/Osadzanie_s%C5%82%C3%B3w" title="Osadzanie słów – Polish" lang="pl" hreflang="pl" data-title="Osadzanie słów" data-language-autonym="Polski" data-language-local-name="Polish" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-pt mw-list-item"><a href="https://pt.wikipedia.org/wiki/Word_embedding" title="Word embedding – Portuguese" lang="pt" hreflang="pt" data-title="Word embedding" data-language-autonym="Português" data-language-local-name="Portuguese" class="interlanguage-link-target"><span>Português</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B2" title="Векторное представление слов – Russian" lang="ru" hreflang="ru" data-title="Векторное представление слов" data-language-autonym="Русский" data-language-local-name="Russian" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-ckb mw-list-item"><a href="https://ckb.wikipedia.org/wiki/%D8%AC%DB%8E%DA%AF%DB%8C%D8%B1%DA%A9%D8%B1%D8%AF%D9%86%DB%8C_%D9%88%D8%B4%DB%95" title="جێگیرکردنی وشە – Central Kurdish" lang="ckb" hreflang="ckb" data-title="جێگیرکردنی وشە" data-language-autonym="کوردی" data-language-local-name="Central Kurdish" class="interlanguage-link-target"><span>کوردی</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/Ugra%C4%91ivanje_re%C4%8Di" title="Ugrađivanje reči – Serbian" lang="sr" hreflang="sr" data-title="Ugrađivanje reči" data-language-autonym="Српски / srpski" data-language-local-name="Serbian" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-th mw-list-item"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%9D%E0%B8%B1%E0%B8%87%E0%B8%84%E0%B8%B3" title="การฝังคำ – Thai" lang="th" hreflang="th" data-title="การฝังคำ" data-language-autonym="ไทย" data-language-local-name="Thai" class="interlanguage-link-target"><span>ไทย</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%92%D0%BA%D0%BB%D0%B0%D0%B4%D0%B0%D0%BD%D0%BD%D1%8F_%D1%81%D0%BB%D1%96%D0%B2" title="Вкладання слів – Ukrainian" lang="uk" hreflang="uk" data-title="Вкладання слів" data-language-autonym="Українська" data-language-local-name="Ukrainian" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/Vect%C6%A1_t%E1%BB%AB" title="Vectơ từ – Vietnamese" lang="vi" hreflang="vi" data-title="Vectơ từ" data-language-autonym="Tiếng Việt" data-language-local-name="Vietnamese" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E8%A9%9E%E5%B5%8C%E5%85%A5" title="詞嵌入 – Cantonese" lang="yue" hreflang="yue" data-title="詞嵌入" data-language-autonym="粵語" data-language-local-name="Cantonese" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E8%AF%8D%E5%B5%8C%E5%85%A5" title="词嵌入 – Chinese" lang="zh" hreflang="zh" data-title="词嵌入" data-language-autonym="中文" data-language-local-name="Chinese" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q18395344#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar vector-feature-custom-font-size-clientpref--excluded">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Word_embedding" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Word_embedding" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Word_embedding"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Word_embedding"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Word_embedding" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Word_embedding" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;oldid=1323019878" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Word_embedding&amp;id=1323019878&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FWord_embedding"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FWord_embedding"><span>Download QR code</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Word_embedding&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Word_embedding&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-wikibase" class="wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q18395344" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end no-font-mode-scale">
					<div class="vector-sticky-pinned-container">
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Method in natural language processing</div>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1246091330">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}}</style><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047488" /><table class="sidebar sidebar-collapse nomobile nowraplinks"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Paradigms</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" class="mw-redirect" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" title="Meta-learning (computer science)">Meta-learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Batch_learning" class="mw-redirect" title="Batch learning">Batch learning</a></li>
<li><a href="/wiki/Curriculum_learning" title="Curriculum learning">Curriculum learning</a></li>
<li><a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">Rule-based learning</a></li>
<li><a href="/wiki/Neuro-symbolic_AI" title="Neuro-symbolic AI">Neuro-symbolic AI</a></li>
<li><a href="/wiki/Neuromorphic_engineering" class="mw-redirect" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Quantum_machine_learning" title="Quantum machine learning">Quantum machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Generative_model" title="Generative model">Generative modeling</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></li>
<li><a href="/wiki/Density_estimation" title="Density estimation">Density estimation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_cleaning" class="mw-redirect" title="Data cleaning">Data cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Multimodal_learning" title="Multimodal learning">Multimodal learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><div style="display: inline-block; line-height: 1.2em; padding: .1em 0;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><span class="nobold"><span style="font-size: 85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">Fuzzy</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">SDL</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Random_sample_consensus" title="Random sample consensus">RANSAC</a></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
<li><a href="/wiki/Isolation_forest" title="Isolation forest">Isolation forest</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Neural networks</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward neural network</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li>
<li><a href="/wiki/Reservoir_computing" title="Reservoir computing">reservoir computing</a></li></ul></li>
<li><a href="/wiki/Boltzmann_machine" title="Boltzmann machine">Boltzmann machine</a>
<ul><li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted</a></li></ul></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Diffusion_model" title="Diffusion model">Diffusion model</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li>
<li><a href="/wiki/LeNet" title="LeNet">LeNet</a></li>
<li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li></ul></li>
<li><a href="/wiki/Neural_field" title="Neural field">Neural field</a>
<ul><li><a href="/wiki/Neural_radiance_field" title="Neural radiance field">Neural radiance field</a></li>
<li><a href="/wiki/Physics-informed_neural_networks" title="Physics-informed neural networks">Physics-informed neural networks</a></li></ul></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent</a>
<ul><li><a href="/wiki/Self-play_(reinforcement_learning_technique)" class="mw-redirect" title="Self-play (reinforcement learning technique)">Self-play</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Learning with humans</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a></li>
<li><a href="/wiki/Crowdsourcing" title="Crowdsourcing">Crowdsourcing</a></li>
<li><a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">Human-in-the-loop</a></li>
<li><a href="/wiki/Mechanistic_interpretability" title="Mechanistic interpretability">Mechanistic interpretability</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Model diagnostics</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Coefficient_of_determination" title="Coefficient of determination">Coefficient of determination</a></li>
<li><a href="/wiki/Confusion_matrix" title="Confusion matrix">Confusion matrix</a></li>
<li><a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">Learning curve</a></li>
<li><a href="/wiki/Receiver_operating_characteristic" title="Receiver operating characteristic">ROC curve</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Mathematical foundations</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
<li><a href="/wiki/Topological_deep_learning" title="Topological deep learning">Topological deep learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Journals and conferences</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/AAAI_Conference_on_Artificial_Intelligence" title="AAAI Conference on Artificial Intelligence">AAAI</a></li>
<li><a href="/wiki/ECML_PKDD" title="ECML PKDD">ECML PKDD</a></li>
<li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/International_Conference_on_Learning_Representations" title="International Conference on Learning Representations">ICLR</a></li>
<li><a href="/wiki/International_Joint_Conference_on_Artificial_Intelligence" title="International Joint Conference on Artificial Intelligence">IJCAI</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a>
<ul><li><a href="/wiki/List_of_datasets_in_computer_vision_and_image_processing" title="List of datasets in computer vision and image processing">List of datasets in computer vision and image processing</a></li></ul></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374" /><style data-mw-deduplicate="TemplateStyles:r1239400231">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Machine_learning" title="Special:EditPage/Template:Machine learning"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Word_embedding_illustration.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Word_embedding_illustration.svg/250px-Word_embedding_illustration.svg.png" decoding="async" width="250" height="232" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Word_embedding_illustration.svg/375px-Word_embedding_illustration.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Word_embedding_illustration.svg/500px-Word_embedding_illustration.svg.png 2x" data-file-width="213" data-file-height="198" /></a><figcaption>Illustration of word embedding. Each word is a point in some space. Word embedding enables a processor to perform semantic operations like obtaining the capital of a given country.</figcaption></figure>
<p>In <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>, a <b>word embedding</b> is a representation of a word. The <a href="/wiki/Embedding_(machine_learning)" title="Embedding (machine learning)">embedding</a> is used in <a href="/wiki/Content_analysis" title="Content analysis">text analysis</a>. Typically, the representation is a <a href="/wiki/Real_number" title="Real number">real-valued</a> vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.<sup id="cite&#95;ref-1" class="reference"><a href="#cite_note-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup> Word embeddings can be obtained using <a href="/wiki/Language_model" title="Language model">language modeling</a> and <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a> techniques, where words or phrases from the vocabulary are mapped to <a href="/wiki/Vector_(mathematics)" class="mw-redirect" title="Vector (mathematics)">vectors</a> of <a href="/wiki/Real_numbers" class="mw-redirect" title="Real numbers">real numbers</a>.
</p><p>Methods to generate this mapping include <a href="/wiki/Neural_net_language_model" class="mw-redirect" title="Neural net language model">neural networks</a>,<sup id="cite&#95;ref-2" class="reference"><a href="#cite_note-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> on the word <a href="/wiki/Co-occurrence_matrix" title="Co-occurrence matrix">co-occurrence matrix</a>,<sup id="cite&#95;ref-3" class="reference"><a href="#cite_note-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">&#91;</span>4<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-5" class="reference"><a href="#cite_note-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup> probabilistic models,<sup id="cite&#95;ref-6" class="reference"><a href="#cite_note-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> explainable knowledge base method,<sup id="cite&#95;ref-7" class="reference"><a href="#cite_note-7"><span class="cite-bracket">&#91;</span>7<span class="cite-bracket">&#93;</span></a></sup> and explicit representation in terms of the context in which words appear.<sup id="cite&#95;ref-8" class="reference"><a href="#cite_note-8"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as <a href="/wiki/Syntactic_parsing" class="mw-redirect" title="Syntactic parsing">syntactic parsing</a><sup id="cite&#95;ref-9" class="reference"><a href="#cite_note-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup> and <a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a>.<sup id="cite&#95;ref-10" class="reference"><a href="#cite_note-10"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup>
</p>
<meta property="mw:PageProp/toc" />
<div class="mw-heading mw-heading2"><h2 id="Development_and_history_of_the_approach">Development and history of the approach</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=1" title="Edit section: Development and history of the approach"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>In <a href="/wiki/Distributional_semantics" title="Distributional semantics">distributional semantics</a>, a quantitative methodological approach for understanding meaning in observed language, word embeddings or semantic <a href="/wiki/Feature_space" class="mw-redirect" title="Feature space">feature space</a> models have been used as a knowledge representation for some time.<sup id="cite&#95;ref-11" class="reference"><a href="#cite_note-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup> Such models aim to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data.  The underlying idea that "a word is characterized by the company it keeps" was proposed in a 1957 article by <a href="/wiki/J._R._Firth" class="mw-redirect" title="J. R. Firth">John Rupert Firth</a>,<sup id="cite&#95;ref-12" class="reference"><a href="#cite_note-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup> but also has roots in the contemporaneous work on search systems<sup id="cite&#95;ref-13" class="reference"><a href="#cite_note-13"><span class="cite-bracket">&#91;</span>13<span class="cite-bracket">&#93;</span></a></sup> and in cognitive psychology.<sup id="cite&#95;ref-14" class="reference"><a href="#cite_note-14"><span class="cite-bracket">&#91;</span>14<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The notion of a semantic space with lexical items (words or multi-word terms) represented as vectors or embeddings is based on the computational challenges of capturing distributional characteristics and using them for practical application to measure similarity between words, phrases, or entire documents. The first generation of semantic space models is the <a href="/wiki/Vector_space_model" title="Vector space model">vector space model</a> for information retrieval.<sup id="cite&#95;ref-Salton&#95;original&#95;15-0" class="reference"><a href="#cite_note-Salton_original-15"><span class="cite-bracket">&#91;</span>15<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-SaltonEA&#95;CACM&#95;16-0" class="reference"><a href="#cite_note-SaltonEA_CACM-16"><span class="cite-bracket">&#91;</span>16<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-17" class="reference"><a href="#cite_note-17"><span class="cite-bracket">&#91;</span>17<span class="cite-bracket">&#93;</span></a></sup> Such vector space models for words and their distributional data implemented in their simplest form results in a very sparse vector space of high dimensionality (cf. <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>). Reducing the number of dimensions using linear algebraic methods such as <a href="/wiki/Singular-value_decomposition" class="mw-redirect" title="Singular-value decomposition">singular value decomposition</a> then led to the introduction of <a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">latent semantic analysis</a> in the late 1980s and the <a href="/wiki/Random_indexing" title="Random indexing">random indexing</a> approach for collecting word co-occurrence contexts.<sup id="cite&#95;ref-18" class="reference"><a href="#cite_note-18"><span class="cite-bracket">&#91;</span>18<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-19" class="reference"><a href="#cite_note-19"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-20" class="reference"><a href="#cite_note-20"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-21" class="reference"><a href="#cite_note-21"><span class="cite-bracket">&#91;</span>21<span class="cite-bracket">&#93;</span></a></sup> In 2000, <a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Bengio</a> et al. provided in a series of papers titled "Neural probabilistic language models" to reduce the high dimensionality of word representations in contexts by "learning a distributed representation for words".<sup id="cite&#95;ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">&#91;</span>22<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-23" class="reference"><a href="#cite_note-23"><span class="cite-bracket">&#91;</span>23<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-24" class="reference"><a href="#cite_note-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup>
</p><p>A study published in <a href="/wiki/NeurIPS" class="mw-redirect" title="NeurIPS">NeurIPS</a> (NIPS) 2002 introduced the use of both word and document embeddings applying the method of kernel CCA to bilingual (and multi-lingual) corpora, also providing an early example of <a href="/wiki/Self-supervised_learning" title="Self-supervised learning">self-supervised learning</a> of word embeddings.<sup id="cite&#95;ref-25" class="reference"><a href="#cite_note-25"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in Lavelli et al., 2004.<sup id="cite&#95;ref-26" class="reference"><a href="#cite_note-26"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup> Roweis and Saul published in <i><a href="/wiki/Science_(journal)" title="Science (journal)">Science</a></i> how to use "<a href="/wiki/Nonlinear_dimensionality_reduction#Locally-linear_embedding" title="Nonlinear dimensionality reduction">locally linear embedding</a>" (LLE) to discover representations of high dimensional data structures.<sup id="cite&#95;ref-27" class="reference"><a href="#cite_note-27"><span class="cite-bracket">&#91;</span>27<span class="cite-bracket">&#93;</span></a></sup> Most new word embedding techniques after about 2005 rely on a <a href="/wiki/Neural_network" title="Neural network">neural network</a> architecture instead of more probabilistic and algebraic models, after foundational work done by Yoshua Bengio<sup id="cite&#95;ref-28" class="reference"><a href="#cite_note-28"><span class="cite-bracket">&#91;</span>28<span class="cite-bracket">&#93;</span></a></sup><sup class="noprint Inline-Template noprint Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Verifiability#Wikipedia_and_sources_that_mirror_or_use_it" title="Wikipedia:Verifiability"><span title="This claim cites another Wikipedia article. Articles need references to reliable third-party sources. (May 2024)">circular reference</span></a></i>&#93;</sup> and colleagues.<sup id="cite&#95;ref-29" class="reference"><a href="#cite_note-29"><span class="cite-bracket">&#91;</span>29<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-30" class="reference"><a href="#cite_note-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The approach has been adopted by many research groups after theoretical advances in 2010 had been made on the quality of vectors and the training speed of the model, as well as after hardware advances allowed for a broader <a href="/wiki/Parameter_space" title="Parameter space">parameter space</a> to be explored profitably. In 2013, a team at <a href="/wiki/Google" title="Google">Google</a> led by <a href="/wiki/Tomas_Mikolov" class="mw-redirect" title="Tomas Mikolov">Tomas Mikolov</a> created <a href="/wiki/Word2vec" title="Word2vec">word2vec</a>, a word embedding toolkit that can train vector space models faster than previous approaches. The word2vec approach has been widely used in experimentation and was instrumental in raising interest for word embeddings as a technology, moving the research strand out of specialised research into broader experimentation and eventually paving the way for practical application.<sup id="cite&#95;ref-31" class="reference"><a href="#cite_note-31"><span class="cite-bracket">&#91;</span>31<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Polysemy_and_homonymy">Polysemy and homonymy</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=2" title="Edit section: Polysemy and homonymy"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Historically, one of the main limitations of static word embeddings or word <a href="/wiki/Vector_space_model" title="Vector space model">vector space models</a> is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, <a href="/wiki/Polysemy" title="Polysemy">polysemy</a> and <a href="/wiki/Homonym" title="Homonym">homonymy</a> are not handled properly. For example, in the sentence "The club I tried yesterday was great!", it is not clear if the term <i>club</i> is related to the word sense of a <i><a href="/wiki/Club_sandwich" title="Club sandwich">club sandwich</a></i>, <i><a href="/wiki/Meeting_house" title="Meeting house">clubhouse</a></i>, <i><a href="/wiki/Golf_club" title="Golf club">golf club</a></i>, or any other sense that <i>club</i> might have. The necessity to accommodate multiple meanings per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split single-sense embeddings into multi-sense ones.<sup id="cite&#95;ref-32" class="reference"><a href="#cite_note-32"><span class="cite-bracket">&#91;</span>32<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-33" class="reference"><a href="#cite_note-33"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Most approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., unsupervised and knowledge-based.<sup id="cite&#95;ref-34" class="reference"><a href="#cite_note-34"><span class="cite-bracket">&#91;</span>34<span class="cite-bracket">&#93;</span></a></sup> Based on <a href="/wiki/Word2vec" title="Word2vec">word2vec</a> skip-gram, Multi-Sense Skip-Gram (MSSG)<sup id="cite&#95;ref-35" class="reference"><a href="#cite_note-35"><span class="cite-bracket">&#91;</span>35<span class="cite-bracket">&#93;</span></a></sup> performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., <a href="/wiki/WordNet" title="WordNet">WordNet</a>, <a href="/wiki/Open_Mind_Common_Sense" title="Open Mind Common Sense">ConceptNet</a>, <a href="/wiki/BabelNet" title="BabelNet">BabelNet</a>), word embeddings and <a href="/wiki/Word_sense_disambiguation" class="mw-redirect" title="Word sense disambiguation">word sense disambiguation</a>, Most Suitable Sense Annotation (MSSA)<sup id="cite&#95;ref-36" class="reference"><a href="#cite_note-36"><span class="cite-bracket">&#91;</span>36<span class="cite-bracket">&#93;</span></a></sup> labels word-senses through an unsupervised and knowledge-based approach, considering a word's context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner.<sup id="cite&#95;ref-37" class="reference"><a href="#cite_note-37"><span class="cite-bracket">&#91;</span>37<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The use of multi-sense embeddings is known to improve performance in several NLP tasks, such as <a href="/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a>, semantic relation identification, <a href="/wiki/Semantic_relatedness" class="mw-redirect" title="Semantic relatedness">semantic relatedness</a>, <a href="/wiki/Named_entity_recognition" class="mw-redirect" title="Named entity recognition">named entity recognition</a> and sentiment analysis.<sup id="cite&#95;ref-:1&#95;38-0" class="reference"><a href="#cite_note-:1-38"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-39" class="reference"><a href="#cite_note-39"><span class="cite-bracket">&#91;</span>39<span class="cite-bracket">&#93;</span></a></sup>
</p><p>As of the late 2010s, contextually-meaningful embeddings such as <a href="/wiki/ELMo" title="ELMo">ELMo</a> and <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a> have been developed.<sup id="cite&#95;ref-40" class="reference"><a href="#cite_note-40"><span class="cite-bracket">&#91;</span>40<span class="cite-bracket">&#93;</span></a></sup> Unlike static word embeddings, these embeddings are at the token-level, in that each occurrence of a word has its own embedding. These embeddings better reflect the multi-sense nature of words, because occurrences of a word in similar contexts are situated in similar regions of BERT's embedding space.<sup id="cite&#95;ref-41" class="reference"><a href="#cite_note-41"><span class="cite-bracket">&#91;</span>41<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-42" class="reference"><a href="#cite_note-42"><span class="cite-bracket">&#91;</span>42<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="For_biological_sequences:_BioVectors">For biological sequences: BioVectors</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=3" title="Edit section: For biological sequences: BioVectors"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Word embeddings for <i>n-</i>grams in biological sequences (e.g. DNA, RNA, and Proteins) for <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a> applications have been proposed by Asgari and Mofrad.<sup id="cite&#95;ref-:0&#95;43-0" class="reference"><a href="#cite_note-:0-43"><span class="cite-bracket">&#91;</span>43<span class="cite-bracket">&#93;</span></a></sup> Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in <a href="/wiki/Proteomics" title="Proteomics">proteomics</a> and <a href="/wiki/Genomics" title="Genomics">genomics</a>. The results presented by Asgari and Mofrad<sup id="cite&#95;ref-:0&#95;43-1" class="reference"><a href="#cite_note-:0-43"><span class="cite-bracket">&#91;</span>43<span class="cite-bracket">&#93;</span></a></sup> suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.
</p>
<div class="mw-heading mw-heading2"><h2 id="Game_design">Game design</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=4" title="Edit section: Game design"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Word embeddings with applications in <a href="/wiki/Game_design" title="Game design">game design</a> have been proposed by Rabii and Cook<sup id="cite&#95;ref-:2&#95;44-0" class="reference"><a href="#cite_note-:2-44"><span class="cite-bracket">&#91;</span>44<span class="cite-bracket">&#93;</span></a></sup> as a way to discover <a href="/wiki/Emergent_gameplay" title="Emergent gameplay">emergent gameplay</a> using logs of gameplay data. The process requires transcribing actions that occur during a game within a <a href="/wiki/Formal_language" title="Formal language">formal language</a> and then using the resulting text to create word embeddings. The results presented by Rabii and Cook<sup id="cite&#95;ref-:2&#95;44-1" class="reference"><a href="#cite_note-:2-44"><span class="cite-bracket">&#91;</span>44<span class="cite-bracket">&#93;</span></a></sup> suggest that the resulting vectors can capture expert knowledge about games like <a href="/wiki/Chess" title="Chess">chess</a> that are not explicitly stated in the game's rules.
</p>
<div class="mw-heading mw-heading2"><h2 id="Sentence_embeddings">Sentence embeddings</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=5" title="Edit section: Sentence embeddings"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1320445320">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Sentence_embedding" title="Sentence embedding">Sentence embedding</a></div>
<p>The idea has been extended to embeddings of entire sentences or even documents, e.g. in the form of the <a href="/wiki/Thought_vector" title="Thought vector">thought vectors</a> concept. In 2015, some researchers suggested "skip-thought vectors" as a means to improve the quality of <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a>.<sup id="cite&#95;ref-45" class="reference"><a href="#cite_note-45"><span class="cite-bracket">&#91;</span>45<span class="cite-bracket">&#93;</span></a></sup> A more recent and popular approach for representing sentences is Sentence-BERT, or SentenceTransformers, which modifies pre-trained <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a> with the use of siamese and triplet network structures.<sup id="cite&#95;ref-46" class="reference"><a href="#cite_note-46"><span class="cite-bracket">&#91;</span>46<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Software">Software</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=6" title="Edit section: Software"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Software for training and using word embeddings includes <a href="/wiki/Tom%C3%A1%C5%A1_Mikolov" title="Tomáš Mikolov">Tomáš Mikolov</a>'s <a href="/wiki/Word2vec" title="Word2vec">Word2vec</a>, Stanford University's <a href="/wiki/GloVe_(machine_learning)" class="mw-redirect" title="GloVe (machine learning)">GloVe</a>,<sup id="cite&#95;ref-47" class="reference"><a href="#cite_note-47"><span class="cite-bracket">&#91;</span>47<span class="cite-bracket">&#93;</span></a></sup> GN-GloVe,<sup id="cite&#95;ref-gn-glove&#95;48-0" class="reference"><a href="#cite_note-gn-glove-48"><span class="cite-bracket">&#91;</span>48<span class="cite-bracket">&#93;</span></a></sup> Flair embeddings,<sup id="cite&#95;ref-:1&#95;38-1" class="reference"><a href="#cite_note-:1-38"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup> AllenNLP's <a href="/wiki/ELMo" title="ELMo">ELMo</a>,<sup id="cite&#95;ref-49" class="reference"><a href="#cite_note-49"><span class="cite-bracket">&#91;</span>49<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a>,<sup id="cite&#95;ref-50" class="reference"><a href="#cite_note-50"><span class="cite-bracket">&#91;</span>50<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/FastText" title="FastText">fastText</a>, <a href="/wiki/Gensim" title="Gensim">Gensim</a>,<sup id="cite&#95;ref-51" class="reference"><a href="#cite_note-51"><span class="cite-bracket">&#91;</span>51<span class="cite-bracket">&#93;</span></a></sup> Indra,<sup id="cite&#95;ref-52" class="reference"><a href="#cite_note-52"><span class="cite-bracket">&#91;</span>52<span class="cite-bracket">&#93;</span></a></sup> and <a href="/wiki/Deeplearning4j" title="Deeplearning4j">Deeplearning4j</a>. <a href="/wiki/Principal_Component_Analysis" class="mw-redirect" title="Principal Component Analysis">Principal Component Analysis</a> (PCA) and <a href="/wiki/T-Distributed_Stochastic_Neighbour_Embedding" class="mw-redirect" title="T-Distributed Stochastic Neighbour Embedding">T-Distributed Stochastic Neighbour Embedding</a> (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and <a href="/wiki/Word-sense_induction" title="Word-sense induction">clusters</a>.<sup id="cite&#95;ref-53" class="reference"><a href="#cite_note-53"><span class="cite-bracket">&#91;</span>53<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Examples_of_application">Examples of application</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=7" title="Edit section: Examples of application"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>For instance, the fastText is also used to calculate word embeddings for <a href="/wiki/Text_corpora" class="mw-redirect" title="Text corpora">text corpora</a> in <a href="/wiki/Sketch_Engine" title="Sketch Engine">Sketch Engine</a> that are available online.<sup id="cite&#95;ref-54" class="reference"><a href="#cite_note-54"><span class="cite-bracket">&#91;</span>54<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Ethical_implications">Ethical implications</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=8" title="Edit section: Ethical implications"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Word embeddings may contain the biases and stereotypes contained in the trained dataset, as Bolukbasi et al. points out in the 2016 paper "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings" that a publicly available (and popular) word2vec embedding trained on <a href="/wiki/Google_News" title="Google News">Google News</a> texts (a commonly used data corpus), which consists of text written by professional journalists, still shows disproportionate word associations reflecting gender and racial biases when extracting word analogies.<sup id="cite&#95;ref-55" class="reference"><a href="#cite_note-55"><span class="cite-bracket">&#91;</span>55<span class="cite-bracket">&#93;</span></a></sup> For example, one of the analogies generated using the aforementioned word embedding is "man is to computer programmer as woman is to homemaker".<sup id="cite&#95;ref-56" class="reference"><a href="#cite_note-56"><span class="cite-bracket">&#91;</span>56<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-57" class="reference"><a href="#cite_note-57"><span class="cite-bracket">&#91;</span>57<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Research done by Jieyu Zhou et al. shows that the applications of these trained word embeddings without careful oversight likely perpetuates existing bias in society, which is introduced through unaltered training data. Furthermore, word embeddings can even amplify these biases.<sup id="cite&#95;ref-58" class="reference"><a href="#cite_note-58"><span class="cite-bracket">&#91;</span>58<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-59" class="reference"><a href="#cite_note-59"><span class="cite-bracket">&#91;</span>59<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=9" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/wiki/Embedding_(machine_learning)" title="Embedding (machine learning)">Embedding (machine learning)</a></li>
<li><a href="/wiki/Brown_clustering" title="Brown clustering">Brown clustering</a></li>
<li><a href="/wiki/Distributional%E2%80%93relational_database" title="Distributional–relational database">Distributional–relational database</a></li></ul>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word_embedding&amp;action=edit&amp;section=10" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1239543626">.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite&#95;note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1238218222">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFJurafskyH.&#95;James2000" class="citation book cs1">Jurafsky, Daniel; H. James, Martin (2000). <a rel="nofollow" class="external text" href="https://web.stanford.edu/~jurafsky/slp3/"><i>Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition</i></a>. Upper Saddle River, N.J.: Prentice Hall. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-13-095069-7" title="Special:BookSources/978-0-13-095069-7"><bdi>978-0-13-095069-7</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Speech+and+language+processing%3A+an+introduction+to+natural+language+processing%2C+computational+linguistics%2C+and+speech+recognition&amp;rft.place=Upper+Saddle+River%2C+N.J.&amp;rft.pub=Prentice+Hall&amp;rft.date=2000&amp;rft.isbn=978-0-13-095069-7&amp;rft.aulast=Jurafsky&amp;rft.aufirst=Daniel&amp;rft.au=H.+James%2C+Martin&amp;rft&#95;id=https%3A%2F%2Fweb.stanford.edu%2F~jurafsky%2Fslp3%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMikolovSutskeverChenCorrado2013" class="citation arxiv cs1">Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg; Dean, Jeffrey (2013). "Distributed Representations of Words and Phrases and their Compositionality". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1310.4546">1310.4546</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Distributed+Representations+of+Words+and+Phrases+and+their+Compositionality&amp;rft.date=2013&amp;rft&#95;id=info%3Aarxiv%2F1310.4546&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tomas&amp;rft.au=Sutskever%2C+Ilya&amp;rft.au=Chen%2C+Kai&amp;rft.au=Corrado%2C+Greg&amp;rft.au=Dean%2C+Jeffrey&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLebretCollobert2013" class="citation book cs1">Lebret, Rémi; Collobert, Ronan (2013). "Word Emdeddings through Hellinger PCA". <i>Conference of the European Chapter of the Association for Computational Linguistics (EACL)</i>. Vol.&#160;2014. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1312.5542">1312.5542</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Word+Emdeddings+through+Hellinger+PCA&amp;rft.btitle=Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics+%28EACL%29&amp;rft.date=2013&amp;rft&#95;id=info%3Aarxiv%2F1312.5542&amp;rft.aulast=Lebret&amp;rft.aufirst=R%C3%A9mi&amp;rft.au=Collobert%2C+Ronan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLevyGoldberg2014" class="citation conference cs1">Levy, Omer; Goldberg, Yoav (2014). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf"><i>Neural Word Embedding as Implicit Matrix Factorization</i></a> <span class="cs1-format">(PDF)</span>. NIPS.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Neural+Word+Embedding+as+Implicit+Matrix+Factorization&amp;rft.date=2014&amp;rft.aulast=Levy&amp;rft.aufirst=Omer&amp;rft.au=Goldberg%2C+Yoav&amp;rft&#95;id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5477-neural-word-embedding-as-implicit-matrix-factorization.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLiXu2015" class="citation conference cs1">Li, Yitan; Xu, Linli (2015). <a rel="nofollow" class="external text" href="http://ijcai.org/papers15/Papers/IJCAI15-513.pdf"><i>Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective</i></a> <span class="cs1-format">(PDF)</span>. Int'l J. Conf. on Artificial Intelligence (IJCAI).</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Word+Embedding+Revisited%3A+A+New+Representation+Learning+and+Explicit+Matrix+Factorization+Perspective&amp;rft.date=2015&amp;rft.aulast=Li&amp;rft.aufirst=Yitan&amp;rft.au=Xu%2C+Linli&amp;rft&#95;id=http%3A%2F%2Fijcai.org%2Fpapers15%2FPapers%2FIJCAI15-513.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGloberson2007" class="citation journal cs1">Globerson, Amir (2007). <a rel="nofollow" class="external text" href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34951.pdf">"Euclidean Embedding of Co-occurrence Data"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Euclidean+Embedding+of+Co-occurrence+Data&amp;rft.date=2007&amp;rft.aulast=Globerson&amp;rft.aufirst=Amir&amp;rft&#95;id=http%3A%2F%2Fstatic.googleusercontent.com%2Fmedia%2Fresearch.google.com%2Fen%2F%2Fpubs%2Farchive%2F34951.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFQureshiGreene2018" class="citation journal cs1">Qureshi, M. Atif; Greene, Derek (2018-06-04). "EVE: explainable vector based embedding technique using Wikipedia". <i>Journal of Intelligent Information Systems</i>. <b>53</b>: <span class="nowrap">137–</span>165. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1702.06891">1702.06891</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10844-018-0511-x">10.1007/s10844-018-0511-x</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0925-9902">0925-9902</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:10656055">10656055</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Intelligent+Information+Systems&amp;rft.atitle=EVE%3A+explainable+vector+based+embedding+technique+using+Wikipedia&amp;rft.volume=53&amp;rft.pages=137-165&amp;rft.date=2018-06-04&amp;rft&#95;id=info%3Aarxiv%2F1702.06891&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A10656055%23id-name%3DS2CID&amp;rft.issn=0925-9902&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs10844-018-0511-x&amp;rft.aulast=Qureshi&amp;rft.aufirst=M.+Atif&amp;rft.au=Greene%2C+Derek&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLevyGoldberg2014" class="citation conference cs1">Levy, Omer; Goldberg, Yoav (2014). <a rel="nofollow" class="external text" href="https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf"><i>Linguistic Regularities in Sparse and Explicit Word Representations</i></a> <span class="cs1-format">(PDF)</span>. CoNLL. pp.&#160;<span class="nowrap">171–</span>180.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Linguistic+Regularities+in+Sparse+and+Explicit+Word+Representations&amp;rft.pages=171-180&amp;rft.date=2014&amp;rft.aulast=Levy&amp;rft.aufirst=Omer&amp;rft.au=Goldberg%2C+Yoav&amp;rft&#95;id=https%3A%2F%2Flevyomer.files.wordpress.com%2F2014%2F04%2Flinguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSocherBauerManningNg2013" class="citation conference cs1">Socher, Richard; Bauer, John; Manning, Christopher; Ng, Andrew (2013). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20160811041232/http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf"><i>Parsing with compositional vector grammars</i></a> <span class="cs1-format">(PDF)</span>. Proc. ACL Conf. Archived from <a rel="nofollow" class="external text" href="http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2016-08-11<span class="reference-accessdate">. Retrieved <span class="nowrap">2014-08-14</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Parsing+with+compositional+vector+grammars&amp;rft.date=2013&amp;rft.aulast=Socher&amp;rft.aufirst=Richard&amp;rft.au=Bauer%2C+John&amp;rft.au=Manning%2C+Christopher&amp;rft.au=Ng%2C+Andrew&amp;rft&#95;id=http%3A%2F%2Fwww.socher.org%2Fuploads%2FMain%2FSocherBauerManningNg&#95;ACL2013.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSocherPerelyginWuChuang2013" class="citation conference cs1">Socher, Richard; Perelygin, Alex; Wu, Jean; Chuang, Jason; Manning, Chris; Ng, Andrew; Potts, Chris (2013). <a rel="nofollow" class="external text" href="http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"><i>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</i></a> <span class="cs1-format">(PDF)</span>. EMNLP.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Recursive+Deep+Models+for+Semantic+Compositionality+Over+a+Sentiment+Treebank&amp;rft.date=2013&amp;rft.aulast=Socher&amp;rft.aufirst=Richard&amp;rft.au=Perelygin%2C+Alex&amp;rft.au=Wu%2C+Jean&amp;rft.au=Chuang%2C+Jason&amp;rft.au=Manning%2C+Chris&amp;rft.au=Ng%2C+Andrew&amp;rft.au=Potts%2C+Chris&amp;rft&#95;id=http%3A%2F%2Fnlp.stanford.edu%2F~socherr%2FEMNLP2013&#95;RNTN.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSahlgren" class="citation web cs1">Sahlgren, Magnus. <a rel="nofollow" class="external text" href="https://www.linkedin.com/pulse/brief-history-word-embeddings-some-clarifications-magnus-sahlgren/">"A brief history of word embeddings"</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=A+brief+history+of+word+embeddings&amp;rft.aulast=Sahlgren&amp;rft.aufirst=Magnus&amp;rft&#95;id=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Fbrief-history-word-embeddings-some-clarifications-magnus-sahlgren%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFFirth1957" class="citation journal cs1">Firth, J.R. (1957). "A synopsis of linguistic theory 1930–1955". <i>Studies in Linguistic Analysis</i>: <span class="nowrap">1–</span>32.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Studies+in+Linguistic+Analysis&amp;rft.atitle=A+synopsis+of+linguistic+theory+1930%E2%80%931955&amp;rft.pages=1-32&amp;rft.date=1957&amp;rft.aulast=Firth&amp;rft.aufirst=J.R.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span> Reprinted in <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFF.R.&#95;Palmer1968" class="citation book cs1">F.R. Palmer, ed. (1968). <i>Selected Papers of J.R. Firth 1952–1959</i>. London: Longman.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Selected+Papers+of+J.R.+Firth+1952%E2%80%931959&amp;rft.pub=London%3A+Longman&amp;rft.date=1968&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_book" title="Template:Cite book">cite book</a>}}</code>:  CS1 maint: publisher location (<a href="/wiki/Category:CS1_maint:_publisher_location" title="Category:CS1 maint: publisher location">link</a>)</span></span>
</li>
<li id="cite&#95;note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLuhn1953" class="citation journal cs1">Luhn, H.P. (1953). "A New Method of Recording and Searching Information". <i>American Documentation</i>. <b>4</b>: <span class="nowrap">14–</span>16. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1002%2Fasi.5090040104">10.1002/asi.5090040104</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Documentation&amp;rft.atitle=A+New+Method+of+Recording+and+Searching+Information&amp;rft.volume=4&amp;rft.pages=14-16&amp;rft.date=1953&amp;rft&#95;id=info%3Adoi%2F10.1002%2Fasi.5090040104&amp;rft.aulast=Luhn&amp;rft.aufirst=H.P.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFOsgoodSuciTannenbaum1957" class="citation book cs1">Osgood, C.E.; Suci, G.J.; Tannenbaum, P.H. (1957). <i>The Measurement of Meaning</i>. University of Illinois Press.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Measurement+of+Meaning.&amp;rft.pub=University+of+Illinois+Press&amp;rft.date=1957&amp;rft.aulast=Osgood&amp;rft.aufirst=C.E.&amp;rft.au=Suci%2C+G.J.&amp;rft.au=Tannenbaum%2C+P.H.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Salton&#95;original-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-Salton_original_15-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSalton1962" class="citation book cs1">Salton, Gerard (1962). "Some experiments in the generation of word and document associations". <i>Proceedings of the December 4-6, 1962, fall joint computer conference on - AFIPS '62 (Fall)</i>. pp.&#160;<span class="nowrap">234–</span>250. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F1461518.1461544">10.1145/1461518.1461544</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4503-7879-6" title="Special:BookSources/978-1-4503-7879-6"><bdi>978-1-4503-7879-6</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:9937095">9937095</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Some+experiments+in+the+generation+of+word+and+document+associations&amp;rft.btitle=Proceedings+of+the+December+4-6%2C+1962%2C+fall+joint+computer+conference+on+-+AFIPS+%2762+%28Fall%29&amp;rft.pages=234-250&amp;rft.date=1962&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A9937095%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1145%2F1461518.1461544&amp;rft.isbn=978-1-4503-7879-6&amp;rft.aulast=Salton&amp;rft.aufirst=Gerard&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span> <span class="cs1-hidden-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_book" title="Template:Cite book">cite book</a>}}</code>: </span><span class="cs1-hidden-error citation-comment">ISBN / Date incompatibility (<a href="/wiki/Help:CS1_errors#invalid_isbn_date" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite&#95;note-SaltonEA&#95;CACM-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-SaltonEA_CACM_16-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSaltonWongYang1975" class="citation journal cs1">Salton, Gerard; Wong, A; Yang, C S (1975). "A Vector Space Model for Automatic Indexing". <i>Communications of the ACM</i>. <b>18</b> (11): <span class="nowrap">613–</span>620. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F361219.361220">10.1145/361219.361220</a>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://hdl.handle.net/1813%2F6057">1813/6057</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:6473756">6473756</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=A+Vector+Space+Model+for+Automatic+Indexing&amp;rft.volume=18&amp;rft.issue=11&amp;rft.pages=613-620&amp;rft.date=1975&amp;rft&#95;id=info%3Ahdl%2F1813%2F6057&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A6473756%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1145%2F361219.361220&amp;rft.aulast=Salton&amp;rft.aufirst=Gerard&amp;rft.au=Wong%2C+A&amp;rft.au=Yang%2C+C+S&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFDubin2004" class="citation web cs1">Dubin, David (2004). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20201018193507/https://www.thefreelibrary.com/The+most+influential+paper+Gerard+Salton+never+wrote.-a0125151308">"The most influential paper Gerard Salton never wrote"</a>. Archived from <a rel="nofollow" class="external text" href="https://www.thefreelibrary.com/The+most+influential+paper+Gerard+Salton+never+wrote.-a0125151308">the original</a> on 18 October 2020<span class="reference-accessdate">. Retrieved <span class="nowrap">18 October</span> 2020</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+most+influential+paper+Gerard+Salton+never+wrote.&amp;rft.date=2004&amp;rft.aulast=Dubin&amp;rft.aufirst=David&amp;rft&#95;id=https%3A%2F%2Fwww.thefreelibrary.com%2FThe%2Bmost%2Binfluential%2Bpaper%2BGerard%2BSalton%2Bnever%2Bwrote.-a0125151308&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Kanerva, Pentti, Kristoferson, Jan and Holst, Anders (2000): <a rel="nofollow" class="external text" href="https://cloudfront.escholarship.org/dist/prd/content/qt5644k0w6/qt5644k0w6.pdf">Random Indexing of Text Samples for Latent Semantic Analysis</a>, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, p.&#160;1036. Mahwah, New Jersey: Erlbaum, 2000.</span>
</li>
<li id="cite&#95;note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFKarlgrenSahlgren2001" class="citation journal cs1">Karlgren, Jussi; Sahlgren, Magnus (2001). Uesaka, Yoshinori; Kanerva, Pentti; Asoh, Hideki (eds.). "From words to understanding". <i>Foundations of Real-World Intelligence</i>. CSLI Publications: <span class="nowrap">294–</span>308.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+of+Real-World+Intelligence&amp;rft.atitle=From+words+to+understanding&amp;rft.pages=294-308&amp;rft.date=2001&amp;rft.aulast=Karlgren&amp;rft.aufirst=Jussi&amp;rft.au=Sahlgren%2C+Magnus&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">Sahlgren, Magnus (2005) <a rel="nofollow" class="external text" href="http://eprints.sics.se/221/1/RI_intro.pdf">An Introduction to Random Indexing</a>, Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005, August 16, Copenhagen, Denmark</span>
</li>
<li id="cite&#95;note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Sahlgren, Magnus, Holst, Anders and Pentti Kanerva (2008) <a rel="nofollow" class="external text" href="http://eprints.sics.se/3436/01/permutationsCogSci08.pdf">Permutations as a Means to Encode Order in Word Space</a>, In Proceedings of the 30th Annual Conference of the Cognitive Science Society: 1300–1305.</span>
</li>
<li id="cite&#95;note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBengioRéjeanPascal2000" class="citation journal cs1">Bengio, Yoshua; Réjean, Ducharme; Pascal, Vincent (2000). <a rel="nofollow" class="external text" href="https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf">"A Neural Probabilistic Language Model"</a> <span class="cs1-format">(PDF)</span>. <i>NeurIPS</i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeurIPS&amp;rft.atitle=A+Neural+Probabilistic+Language+Model&amp;rft.date=2000&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=R%C3%A9jean%2C+Ducharme&amp;rft.au=Pascal%2C+Vincent&amp;rft&#95;id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper&#95;files%2Fpaper%2F2000%2Ffile%2F728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBengioDucharmeVincentJauvin2003" class="citation journal cs1"><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Bengio, Yoshua</a>; Ducharme, Réjean; Vincent, Pascal; Jauvin, Christian (2003). <a rel="nofollow" class="external text" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">"A Neural Probabilistic Language Model"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>3</b>: <span class="nowrap">1137–</span>1155.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=A+Neural+Probabilistic+Language+Model&amp;rft.volume=3&amp;rft.pages=1137-1155&amp;rft.date=2003&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Ducharme%2C+R%C3%A9jean&amp;rft.au=Vincent%2C+Pascal&amp;rft.au=Jauvin%2C+Christian&amp;rft&#95;id=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBengioSchwenkSenécalMorin2006" class="citation book cs1">Bengio, Yoshua; Schwenk, Holger; Senécal, Jean-Sébastien; Morin, Fréderic; Gauvain, Jean-Luc (2006). "A Neural Probabilistic Language Model". <i>Studies in Fuzziness and Soft Computing</i>. Vol.&#160;194. Springer. pp.&#160;<span class="nowrap">137–</span>186. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F3-540-33486-6_6">10.1007/3-540-33486-6_6</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-540-30609-2" title="Special:BookSources/978-3-540-30609-2"><bdi>978-3-540-30609-2</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Neural+Probabilistic+Language+Model&amp;rft.btitle=Studies+in+Fuzziness+and+Soft+Computing&amp;rft.pages=137-186&amp;rft.pub=Springer&amp;rft.date=2006&amp;rft&#95;id=info%3Adoi%2F10.1007%2F3-540-33486-6&#95;6&amp;rft.isbn=978-3-540-30609-2&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Schwenk%2C+Holger&amp;rft.au=Sen%C3%A9cal%2C+Jean-S%C3%A9bastien&amp;rft.au=Morin%2C+Fr%C3%A9deric&amp;rft.au=Gauvain%2C+Jean-Luc&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFVinkourovCristianiniShawe-Taylor2002" class="citation conference cs1">Vinkourov, Alexei; Cristianini, Nello; Shawe-Taylor, John (2002). <a rel="nofollow" class="external text" href="https://proceedings.neurips.cc/paper/2002/file/d5e2fbef30a4eb668a203060ec8e5eef-Paper.pdf"><i>Inferring a semantic representation of text via cross-language correlation analysis</i></a> <span class="cs1-format">(PDF)</span>. Advances in Neural Information Processing Systems. Vol.&#160;15.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Inferring+a+semantic+representation+of+text+via+cross-language+correlation+analysis.&amp;rft.date=2002&amp;rft.aulast=Vinkourov&amp;rft.aufirst=Alexei&amp;rft.au=Cristianini%2C+Nello&amp;rft.au=Shawe-Taylor%2C+John&amp;rft&#95;id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2002%2Ffile%2Fd5e2fbef30a4eb668a203060ec8e5eef-Paper.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLavelliSebastianiZanoli2004" class="citation conference cs1">Lavelli, Alberto; Sebastiani, Fabrizio; Zanoli, Roberto (2004). <i>Distributional term representations: an experimental comparison</i>. 13th ACM International Conference on Information and Knowledge Management. pp.&#160;<span class="nowrap">615–</span>624. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F1031171.1031284">10.1145/1031171.1031284</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Distributional+term+representations%3A+an+experimental+comparison&amp;rft.pages=615-624&amp;rft.date=2004&amp;rft&#95;id=info%3Adoi%2F10.1145%2F1031171.1031284&amp;rft.aulast=Lavelli&amp;rft.aufirst=Alberto&amp;rft.au=Sebastiani%2C+Fabrizio&amp;rft.au=Zanoli%2C+Roberto&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFRoweisSaul2000" class="citation journal cs1">Roweis, Sam T.; Saul, Lawrence K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". <i>Science</i>. <b>290</b> (5500): <span class="nowrap">2323–</span>6. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2000Sci...290.2323R">2000Sci...290.2323R</a>. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.3313">10.1.1.111.3313</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126%2Fscience.290.5500.2323">10.1126/science.290.5500.2323</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/11125150">11125150</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:5987139">5987139</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Nonlinear+Dimensionality+Reduction+by+Locally+Linear+Embedding&amp;rft.volume=290&amp;rft.issue=5500&amp;rft.pages=2323-6&amp;rft.date=2000&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5987139%23id-name%3DS2CID&amp;rft&#95;id=info%3Abibcode%2F2000Sci...290.2323R&amp;rft&#95;id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.111.3313%23id-name%3DCiteSeerX&amp;rft&#95;id=info%3Apmid%2F11125150&amp;rft&#95;id=info%3Adoi%2F10.1126%2Fscience.290.5500.2323&amp;rft.aulast=Roweis&amp;rft.aufirst=Sam+T.&amp;rft.au=Saul%2C+Lawrence+K.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><a href="https://he.wikipedia.org/wiki/%D7%99%D7%94%D7%95%D7%A9%D7%A2_%D7%91%D7%A0%D7%92%27%D7%99%D7%95" class="extiw" title="he:יהושע בנג&#39;יו">he:יהושע בנג'יו</a></span>
</li>
<li id="cite&#95;note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMorinBengio2005" class="citation book cs1">Morin, Fredric; Bengio, Yoshua (2005). <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/r5/morin05a/morin05a.pdf">"Hierarchical probabilistic neural network language model"</a> <span class="cs1-format">(PDF)</span>. In Cowell, Robert G.; Ghahramani, Zoubin (eds.). <i>Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</i>. Proceedings of Machine Learning Research. Vol.&#160;R5. pp.&#160;<span class="nowrap">246–</span>252.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Hierarchical+probabilistic+neural+network+language+model&amp;rft.btitle=Proceedings+of+the+Tenth+International+Workshop+on+Artificial+Intelligence+and+Statistics&amp;rft.series=Proceedings+of+Machine+Learning+Research&amp;rft.pages=246-252&amp;rft.date=2005&amp;rft.aulast=Morin&amp;rft.aufirst=Fredric&amp;rft.au=Bengio%2C+Yoshua&amp;rft&#95;id=http%3A%2F%2Fproceedings.mlr.press%2Fr5%2Fmorin05a%2Fmorin05a.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMnihHinton2009" class="citation journal cs1 cs1-prop-long-vol">Mnih, Andriy; Hinton, Geoffrey (2009). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model">"A Scalable Hierarchical Distributed Language Model"</a>. <i>Advances in Neural Information Processing Systems</i>. 21 (NIPS 2008). Curran Associates, Inc.: <span class="nowrap">1081–</span>1088.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=A+Scalable+Hierarchical+Distributed+Language+Model&amp;rft.volume=21+%28NIPS+2008%29&amp;rft.pages=1081-1088&amp;rft.date=2009&amp;rft.aulast=Mnih&amp;rft.aufirst=Andriy&amp;rft.au=Hinton%2C+Geoffrey&amp;rft&#95;id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3583-a-scalable-hierarchical-distributed-language-model&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://code.google.com/archive/p/word2vec/">"word2vec"</a>. <i>Google Code Archive</i><span class="reference-accessdate">. Retrieved <span class="nowrap">23 July</span> 2021</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+Code+Archive&amp;rft.atitle=word2vec&amp;rft&#95;id=https%3A%2F%2Fcode.google.com%2Farchive%2Fp%2Fword2vec%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFReisingerMooney2010" class="citation book cs1 cs1-prop-long-vol">Reisinger, Joseph; Mooney, Raymond J. (2010). <a rel="nofollow" class="external text" href="https://www.aclweb.org/anthology/N10-1013/"><i>Multi-Prototype Vector-Space Models of Word Meaning</i></a>. Vol.&#160;Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Los Angeles, California: Association for Computational Linguistics. pp.&#160;<span class="nowrap">109–</span>117. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-932432-65-7" title="Special:BookSources/978-1-932432-65-7"><bdi>978-1-932432-65-7</bdi></a><span class="reference-accessdate">. Retrieved <span class="nowrap">October 25,</span> 2019</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Multi-Prototype+Vector-Space+Models+of+Word+Meaning&amp;rft.place=Los+Angeles%2C+California&amp;rft.pages=109-117&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2010&amp;rft.isbn=978-1-932432-65-7&amp;rft.aulast=Reisinger&amp;rft.aufirst=Joseph&amp;rft.au=Mooney%2C+Raymond+J.&amp;rft&#95;id=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN10-1013%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFHuang,&#95;Eric.2012" class="citation book cs1">Huang, Eric. (2012). <i>Improving word representations via global context and multiple word prototypes</i>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/oclc/857900050">857900050</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Improving+word+representations+via+global+context+and+multiple+word+prototypes&amp;rft.date=2012&amp;rft&#95;id=info%3Aoclcnum%2F857900050&amp;rft.au=Huang%2C+Eric.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFCamacho-ColladosPilehvar2018" class="citation arxiv cs1">Camacho-Collados, Jose; Pilehvar, Mohammad Taher (2018). "From Word to Sense Embeddings: A Survey on Vector Representations of Meaning". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1805.04032">1805.04032</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=From+Word+to+Sense+Embeddings%3A+A+Survey+on+Vector+Representations+of+Meaning&amp;rft.date=2018&amp;rft&#95;id=info%3Aarxiv%2F1805.04032&amp;rft.aulast=Camacho-Collados&amp;rft.aufirst=Jose&amp;rft.au=Pilehvar%2C+Mohammad+Taher&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFNeelakantanShankarPassosMcCallum2014" class="citation book cs1">Neelakantan, Arvind; Shankar, Jeevan; Passos, Alexandre; McCallum, Andrew (2014). "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space". <i>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>. Stroudsburg, PA, USA: Association for Computational Linguistics. pp.&#160;<span class="nowrap">1059–</span>1069. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1504.06654">1504.06654</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3115%2Fv1%2Fd14-1113">10.3115/v1/d14-1113</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:15251438">15251438</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Efficient+Non-parametric+Estimation+of+Multiple+Embeddings+per+Word+in+Vector+Space&amp;rft.btitle=Proceedings+of+the+2014+Conference+on+Empirical+Methods+in+Natural+Language+Processing+%28EMNLP%29&amp;rft.place=Stroudsburg%2C+PA%2C+USA&amp;rft.pages=1059-1069&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2014&amp;rft&#95;id=info%3Aarxiv%2F1504.06654&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A15251438%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.3115%2Fv1%2Fd14-1113&amp;rft.aulast=Neelakantan&amp;rft.aufirst=Arvind&amp;rft.au=Shankar%2C+Jeevan&amp;rft.au=Passos%2C+Alexandre&amp;rft.au=McCallum%2C+Andrew&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFRuasGroskyAizawa2019" class="citation journal cs1">Ruas, Terry; Grosky, William; Aizawa, Akiko (2019-12-01). "Multi-sense embeddings through a word sense disambiguation process". <i>Expert Systems with Applications</i>. <b>136</b>: <span class="nowrap">288–</span>303. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2101.08700">2101.08700</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.eswa.2019.06.026">10.1016/j.eswa.2019.06.026</a>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://hdl.handle.net/2027.42%2F145475">2027.42/145475</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0957-4174">0957-4174</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:52225306">52225306</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Expert+Systems+with+Applications&amp;rft.atitle=Multi-sense+embeddings+through+a+word+sense+disambiguation+process&amp;rft.volume=136&amp;rft.pages=288-303&amp;rft.date=2019-12-01&amp;rft&#95;id=info%3Ahdl%2F2027.42%2F145475&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A52225306%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.eswa.2019.06.026&amp;rft&#95;id=info%3Aarxiv%2F2101.08700&amp;rft.issn=0957-4174&amp;rft.aulast=Ruas&amp;rft.aufirst=Terry&amp;rft.au=Grosky%2C+William&amp;rft.au=Aizawa%2C+Akiko&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFAgrePetrovKeskinova2019" class="citation journal cs1">Agre, Gennady; Petrov, Daniel; Keskinova, Simona (2019-03-01). <a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Finfo10030097">"Word Sense Disambiguation Studio: A Flexible System for WSD Feature Extraction"</a>. <i>Information</i>. <b>10</b> (3): 97. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Finfo10030097">10.3390/info10030097</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/2078-2489">2078-2489</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information&amp;rft.atitle=Word+Sense+Disambiguation+Studio%3A+A+Flexible+System+for+WSD+Feature+Extraction&amp;rft.volume=10&amp;rft.issue=3&amp;rft.pages=97&amp;rft.date=2019-03-01&amp;rft&#95;id=info%3Adoi%2F10.3390%2Finfo10030097&amp;rft.issn=2078-2489&amp;rft.aulast=Agre&amp;rft.aufirst=Gennady&amp;rft.au=Petrov%2C+Daniel&amp;rft.au=Keskinova%2C+Simona&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.3390%252Finfo10030097&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:1-38"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_38-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_38-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFAkbikBlytheVollgraf2018" class="citation journal cs1">Akbik, Alan; Blythe, Duncan; Vollgraf, Roland (2018). <a rel="nofollow" class="external text" href="https://www.aclweb.org/anthology/C18-1139">"Contextual String Embeddings for Sequence Labeling"</a>. <i>Proceedings of the 27th International Conference on Computational Linguistics</i>. Santa Fe, New Mexico, USA: Association for Computational Linguistics: <span class="nowrap">1638–</span>1649.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+27th+International+Conference+on+Computational+Linguistics&amp;rft.atitle=Contextual+String+Embeddings+for+Sequence+Labeling&amp;rft.pages=1638-1649&amp;rft.date=2018&amp;rft.aulast=Akbik&amp;rft.aufirst=Alan&amp;rft.au=Blythe%2C+Duncan&amp;rft.au=Vollgraf%2C+Roland&amp;rft&#95;id=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FC18-1139&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLiJurafsky2015" class="citation book cs1">Li, Jiwei; Jurafsky, Dan (2015). "Do Multi-Sense Embeddings Improve Natural Language Understanding?". <i>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</i>. Stroudsburg, PA, USA: Association for Computational Linguistics. pp.&#160;<span class="nowrap">1722–</span>1732. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1506.01070">1506.01070</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2Fd15-1200">10.18653/v1/d15-1200</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:6222768">6222768</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Do+Multi-Sense+Embeddings+Improve+Natural+Language+Understanding%3F&amp;rft.btitle=Proceedings+of+the+2015+Conference+on+Empirical+Methods+in+Natural+Language+Processing&amp;rft.place=Stroudsburg%2C+PA%2C+USA&amp;rft.pages=1722-1732&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2015&amp;rft&#95;id=info%3Aarxiv%2F1506.01070&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A6222768%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2Fd15-1200&amp;rft.aulast=Li&amp;rft.aufirst=Jiwei&amp;rft.au=Jurafsky%2C+Dan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFDevlinChangLeeToutanova2019" class="citation journal cs1">Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (June 2019). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://aclanthology.org/N19-1423/">"Proceedings of the 2019 Conference of the North"</a></span>. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i>. Association for Computational Linguistics: <span class="nowrap">4171–</span>4186. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2FN19-1423">10.18653/v1/N19-1423</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:52967399">52967399</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies%2C+Volume+1+%28Long+and+Short+Papers%29&amp;rft.atitle=Proceedings+of+the+2019+Conference+of+the+North&amp;rft.pages=4171-4186&amp;rft.date=2019-06&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2FN19-1423&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A52967399%23id-name%3DS2CID&amp;rft.aulast=Devlin&amp;rft.aufirst=Jacob&amp;rft.au=Chang%2C+Ming-Wei&amp;rft.au=Lee%2C+Kenton&amp;rft.au=Toutanova%2C+Kristina&amp;rft&#95;id=https%3A%2F%2Faclanthology.org%2FN19-1423%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text">Lucy, Li, and David Bamman. "Characterizing English variation across social media communities with BERT." Transactions of the Association for Computational Linguistics 9 (2021): 538-556.</span>
</li>
<li id="cite&#95;note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text">Reif, Emily, Ann Yuan, Martin Wattenberg, Fernanda B. Viegas, Andy Coenen, Adam Pearce, and Been Kim. "Visualizing and measuring the geometry of BERT." Advances in Neural Information Processing Systems 32 (2019).</span>
</li>
<li id="cite&#95;note-:0-43"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_43-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_43-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFAsgariMofrad2015" class="citation journal cs1">Asgari, Ehsaneddin; Mofrad, Mohammad R.K. (2015). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716">"Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics"</a>. <i>PLOS ONE</i>. <b>10</b> (11) e0141287. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1503.05140">1503.05140</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2015PLoSO..1041287A">2015PLoSO..1041287A</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1371%2Fjournal.pone.0141287">10.1371/journal.pone.0141287</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716">4640716</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/26555596">26555596</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+ONE&amp;rft.atitle=Continuous+Distributed+Representation+of+Biological+Sequences+for+Deep+Proteomics+and+Genomics&amp;rft.volume=10&amp;rft.issue=11&amp;rft.artnum=e0141287&amp;rft.date=2015&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4640716%23id-name%3DPMC&amp;rft&#95;id=info%3Abibcode%2F2015PLoSO..1041287A&amp;rft&#95;id=info%3Aarxiv%2F1503.05140&amp;rft&#95;id=info%3Apmid%2F26555596&amp;rft&#95;id=info%3Adoi%2F10.1371%2Fjournal.pone.0141287&amp;rft.aulast=Asgari&amp;rft.aufirst=Ehsaneddin&amp;rft.au=Mofrad%2C+Mohammad+R.K.&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4640716&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:2-44"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_44-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_44-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFRabiiCook2021" class="citation journal cs1">Rabii, Younès; Cook, Michael (2021-10-04). <a rel="nofollow" class="external text" href="https://ojs.aaai.org/index.php/AIIDE/article/view/18907">"Revealing Game Dynamics via Word Embeddings of Gameplay Data"</a>. <i>Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</i>. <b>17</b> (1): <span class="nowrap">187–</span>194. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1609%2Faiide.v17i1.18907">10.1609/aiide.v17i1.18907</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/2334-0924">2334-0924</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:248175634">248175634</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+and+Interactive+Digital+Entertainment&amp;rft.atitle=Revealing+Game+Dynamics+via+Word+Embeddings+of+Gameplay+Data&amp;rft.volume=17&amp;rft.issue=1&amp;rft.pages=187-194&amp;rft.date=2021-10-04&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A248175634%23id-name%3DS2CID&amp;rft.issn=2334-0924&amp;rft&#95;id=info%3Adoi%2F10.1609%2Faiide.v17i1.18907&amp;rft.aulast=Rabii&amp;rft.aufirst=Youn%C3%A8s&amp;rft.au=Cook%2C+Michael&amp;rft&#95;id=https%3A%2F%2Fojs.aaai.org%2Findex.php%2FAIIDE%2Farticle%2Fview%2F18907&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFKirosZhuSalakhutdinovZemel2015" class="citation arxiv cs1">Kiros, Ryan; Zhu, Yukun; Salakhutdinov, Ruslan; Zemel, Richard S.; Torralba, Antonio; Urtasun, Raquel; Fidler, Sanja (2015). "skip-thought vectors". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1506.06726">1506.06726</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=skip-thought+vectors&amp;rft.date=2015&amp;rft&#95;id=info%3Aarxiv%2F1506.06726&amp;rft.aulast=Kiros&amp;rft.aufirst=Ryan&amp;rft.au=Zhu%2C+Yukun&amp;rft.au=Salakhutdinov%2C+Ruslan&amp;rft.au=Zemel%2C+Richard+S.&amp;rft.au=Torralba%2C+Antonio&amp;rft.au=Urtasun%2C+Raquel&amp;rft.au=Fidler%2C+Sanja&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text">Reimers, Nils, and Iryna Gurevych. "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982-3992. 2019.</span>
</li>
<li id="cite&#95;note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://nlp.stanford.edu/projects/glove/">"GloVe"</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=GloVe&amp;rft&#95;id=http%3A%2F%2Fnlp.stanford.edu%2Fprojects%2Fglove%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-gn-glove-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-gn-glove_48-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFZhao2018" class="citation arxiv cs1">Zhao, Jieyu; et&#160;al. (2018) (2018). "Learning Gender-Neutral Word Embeddings". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1809.01496">1809.01496</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Learning+Gender-Neutral+Word+Embeddings&amp;rft.date=2018&amp;rft&#95;id=info%3Aarxiv%2F1809.01496&amp;rft.aulast=Zhao&amp;rft.aufirst=Jieyu&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://allennlp.org/elmo">"Elmo"</a>. 16 October 2024.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Elmo&amp;rft.date=2024-10-16&amp;rft&#95;id=https%3A%2F%2Fallennlp.org%2Felmo&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFPiresSchlingerGarrette2019" class="citation arxiv cs1">Pires, Telmo; Schlinger, Eva; Garrette, Dan (2019-06-04). "How multilingual is Multilingual BERT?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1906.01502">1906.01502</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=How+multilingual+is+Multilingual+BERT%3F&amp;rft.date=2019-06-04&amp;rft&#95;id=info%3Aarxiv%2F1906.01502&amp;rft.aulast=Pires&amp;rft.aufirst=Telmo&amp;rft.au=Schlinger%2C+Eva&amp;rft.au=Garrette%2C+Dan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://radimrehurek.com/gensim/">"Gensim"</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Gensim&amp;rft&#95;id=http%3A%2F%2Fradimrehurek.com%2Fgensim%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://github.com/Lambda-3/Indra">"Indra"</a>. <i><a href="/wiki/GitHub" title="GitHub">GitHub</a></i>. 2018-10-25.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=Indra&amp;rft.date=2018-10-25&amp;rft&#95;id=https%3A%2F%2Fgithub.com%2FLambda-3%2FIndra&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGhassemiMarkNemati2015" class="citation book cs1">Ghassemi, Mohammad; Mark, Roger; Nemati, Shamim (2015). <a rel="nofollow" class="external text" href="http://www.cinc.org/archives/2015/pdf/0629.pdf">"A visualization of evolving clinical sentiment using vector representations of clinical notes"</a> <span class="cs1-format">(PDF)</span>. <i>2015 Computing in Cardiology Conference (CinC)</i>. Vol.&#160;2015. pp.&#160;<span class="nowrap">629–</span>632. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCIC.2015.7410989">10.1109/CIC.2015.7410989</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5090-0685-4" title="Special:BookSources/978-1-5090-0685-4"><bdi>978-1-5090-0685-4</bdi></a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5070922">5070922</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/27774487">27774487</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+visualization+of+evolving+clinical+sentiment+using+vector+representations+of+clinical+notes&amp;rft.btitle=2015+Computing+in+Cardiology+Conference+%28CinC%29&amp;rft.pages=629-632&amp;rft.date=2015&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5070922%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F27774487&amp;rft&#95;id=info%3Adoi%2F10.1109%2FCIC.2015.7410989&amp;rft.isbn=978-1-5090-0685-4&amp;rft.aulast=Ghassemi&amp;rft.aufirst=Mohammad&amp;rft.au=Mark%2C+Roger&amp;rft.au=Nemati%2C+Shamim&amp;rft&#95;id=http%3A%2F%2Fwww.cinc.org%2Farchives%2F2015%2Fpdf%2F0629.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20180208004241/https://embeddings.sketchengine.co.uk/">"Embedding Viewer"</a>. <i>Embedding Viewer</i>. Lexical Computing. Archived from <a rel="nofollow" class="external text" href="https://embeddings.sketchengine.co.uk/">the original</a> on 8 February 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">7 Feb</span> 2018</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Embedding+Viewer&amp;rft.atitle=Embedding+Viewer&amp;rft&#95;id=https%3A%2F%2Fembeddings.sketchengine.co.uk%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBolukbasiChangZouSaligrama2016" class="citation arxiv cs1">Bolukbasi, Tolga; Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam (2016). "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1607.06520">1607.06520</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Man+is+to+Computer+Programmer+as+Woman+is+to+Homemaker%3F+Debiasing+Word+Embeddings&amp;rft.date=2016&amp;rft&#95;id=info%3Aarxiv%2F1607.06520&amp;rft.aulast=Bolukbasi&amp;rft.aufirst=Tolga&amp;rft.au=Chang%2C+Kai-Wei&amp;rft.au=Zou%2C+James&amp;rft.au=Saligrama%2C+Venkatesh&amp;rft.au=Kalai%2C+Adam&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBolukbasiChangZouSaligrama2016" class="citation arxiv cs1">Bolukbasi, Tolga; Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam (2016-07-21). "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1607.06520">1607.06520</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Man+is+to+Computer+Programmer+as+Woman+is+to+Homemaker%3F+Debiasing+Word+Embeddings&amp;rft.date=2016-07-21&amp;rft&#95;id=info%3Aarxiv%2F1607.06520&amp;rft.aulast=Bolukbasi&amp;rft.aufirst=Tolga&amp;rft.au=Chang%2C+Kai-Wei&amp;rft.au=Zou%2C+James&amp;rft.au=Saligrama%2C+Venkatesh&amp;rft.au=Kalai%2C+Adam&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFDiengRuizBlei2020" class="citation journal cs1">Dieng, Adji B.; Ruiz, Francisco J. R.; Blei, David M. (2020). <a rel="nofollow" class="external text" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00325/96463/Topic-Modeling-in-Embedding-Spaces">"Topic Modeling in Embedding Spaces"</a>. <i>Transactions of the Association for Computational Linguistics</i>. <b>8</b>: <span class="nowrap">439–</span>453. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1907.04907">1907.04907</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Ftacl_a_00325">10.1162/tacl_a_00325</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Transactions+of+the+Association+for+Computational+Linguistics&amp;rft.atitle=Topic+Modeling+in+Embedding+Spaces&amp;rft.volume=8&amp;rft.pages=439-453&amp;rft.date=2020&amp;rft&#95;id=info%3Aarxiv%2F1907.04907&amp;rft&#95;id=info%3Adoi%2F10.1162%2Ftacl&#95;a&#95;00325&amp;rft.aulast=Dieng&amp;rft.aufirst=Adji+B.&amp;rft.au=Ruiz%2C+Francisco+J.+R.&amp;rft.au=Blei%2C+David+M.&amp;rft&#95;id=https%3A%2F%2Fdirect.mit.edu%2Ftacl%2Farticle%2Fdoi%2F10.1162%2Ftacl&#95;a&#95;00325%2F96463%2FTopic-Modeling-in-Embedding-Spaces&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFZhaoWangYatskarOrdonez2017" class="citation book cs1">Zhao, Jieyu; Wang, Tianlu; Yatskar, Mark; Ordonez, Vicente; Chang, Kai-Wei (2017). <a rel="nofollow" class="external text" href="https://aclanthology.org/D17-1323/">"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints"</a>. <i>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</i>. pp.&#160;<span class="nowrap">2979–</span>2989. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2FD17-1323">10.18653/v1/D17-1323</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Men+Also+Like+Shopping%3A+Reducing+Gender+Bias+Amplification+using+Corpus-level+Constraints&amp;rft.btitle=Proceedings+of+the+2017+Conference+on+Empirical+Methods+in+Natural+Language+Processing&amp;rft.pages=2979-2989&amp;rft.date=2017&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2FD17-1323&amp;rft.aulast=Zhao&amp;rft.aufirst=Jieyu&amp;rft.au=Wang%2C+Tianlu&amp;rft.au=Yatskar%2C+Mark&amp;rft.au=Ordonez%2C+Vicente&amp;rft.au=Chang%2C+Kai-Wei&amp;rft&#95;id=https%3A%2F%2Faclanthology.org%2FD17-1323%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFPetreskiHashim2022" class="citation journal cs1">Petreski, Davor; Hashim, Ibrahim C. (2022-05-26). <a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00146-022-01443-w">"Word embeddings are biased. But whose bias are they reflecting?"</a>. <i>AI &amp; Society</i>. <b>38</b> (2): <span class="nowrap">975–</span>982. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00146-022-01443-w">10.1007/s00146-022-01443-w</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1435-5655">1435-5655</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:249112516">249112516</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+%26+Society&amp;rft.atitle=Word+embeddings+are+biased.+But+whose+bias+are+they+reflecting%3F&amp;rft.volume=38&amp;rft.issue=2&amp;rft.pages=975-982&amp;rft.date=2022-05-26&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A249112516%23id-name%3DS2CID&amp;rft.issn=1435-5655&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs00146-022-01443-w&amp;rft.aulast=Petreski&amp;rft.aufirst=Davor&amp;rft.au=Hashim%2C+Ibrahim+C.&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1007%252Fs00146-022-01443-w&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AWord+embedding" class="Z3988"></span></span>
</li>
</ol></div></div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374" /><style data-mw-deduplicate="TemplateStyles:r1314944253">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd;color:inherit}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf;color:inherit}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf;color:inherit}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff;color:inherit}.mw-parser-output .navbox-even{background-color:#f7f7f7;color:inherit}.mw-parser-output .navbox-odd{background-color:transparent;color:inherit}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role="navigation" class="navbox" aria-labelledby="Natural&#95;language&#95;processing4500" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374" /><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231" /><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Natural_language_processing" title="Template:Natural language processing"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Natural_language_processing" title="Template talk:Natural language processing"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Natural_language_processing" title="Special:EditPage/Template:Natural language processing"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Natural&#95;language&#95;processing4500" style="font-size:114%;margin:0 4em"><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">General terms</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AI-complete" title="AI-complete">AI-complete</a></li>
<li><a href="/wiki/Bag-of-words_model" title="Bag-of-words model">Bag-of-words</a></li>
<li><a href="/wiki/N-gram" title="N-gram"><i>n</i>-gram</a>
<ul><li><a href="/wiki/Bigram" title="Bigram">Bigram</a></li>
<li><a href="/wiki/Trigram" title="Trigram">Trigram</a></li></ul></li>
<li><a href="/wiki/Computational_linguistics" title="Computational linguistics">Computational linguistics</a></li>
<li><a href="/wiki/Natural_language_understanding" title="Natural language understanding">Natural language understanding</a></li>
<li><a href="/wiki/Stop_word" title="Stop word">Stop words</a></li>
<li><a href="/wiki/Text_processing" title="Text processing">Text processing</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Text_mining" title="Text mining">Text analysis</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Argument_mining" title="Argument mining">Argument mining</a></li>
<li><a href="/wiki/Collocation_extraction" title="Collocation extraction">Collocation extraction</a></li>
<li><a href="/wiki/Concept_mining" title="Concept mining">Concept mining</a></li>
<li><a href="/wiki/Coreference#Coreference_resolution" title="Coreference">Coreference resolution</a></li>
<li><a href="/wiki/Deep_linguistic_processing" title="Deep linguistic processing">Deep linguistic processing</a></li>
<li><a href="/wiki/Distant_reading" title="Distant reading">Distant reading</a></li>
<li><a href="/wiki/Information_extraction" title="Information extraction">Information extraction</a></li>
<li><a href="/wiki/Named-entity_recognition" title="Named-entity recognition">Named-entity recognition</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Parsing" title="Parsing">Parsing</a>
<ul><li><a href="/wiki/Semantic_parsing" title="Semantic parsing">semantic</a></li>
<li><a href="/wiki/Syntactic_parsing_(computational_linguistics)" title="Syntactic parsing (computational linguistics)">syntactic</a></li></ul></li>
<li><a href="/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">Part-of-speech tagging</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Semantic_role_labeling" title="Semantic role labeling">Semantic role labeling</a></li>
<li><a href="/wiki/Semantic_decomposition_(natural_language_processing)" title="Semantic decomposition (natural language processing)">Semantic decomposition</a></li>
<li><a href="/wiki/Semantic_similarity" title="Semantic similarity">Semantic similarity</a></li>
<li><a href="/wiki/Sentiment_analysis" title="Sentiment analysis">Sentiment analysis</a></li></ul>
<ul><li><a href="/wiki/Terminology_extraction" title="Terminology extraction">Terminology extraction</a></li>
<li><a href="/wiki/Text_mining" title="Text mining">Text mining</a></li>
<li><a href="/wiki/Textual_entailment" title="Textual entailment">Textual entailment</a></li>
<li><a href="/wiki/Truecasing" title="Truecasing">Truecasing</a></li>
<li><a href="/wiki/Word-sense_disambiguation" title="Word-sense disambiguation">Word-sense disambiguation</a></li>
<li><a href="/wiki/Word-sense_induction" title="Word-sense induction">Word-sense induction</a></li></ul>
</div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th id="Text&#95;segmentation252" scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Text_segmentation" title="Text segmentation">Text segmentation</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Compound-term_processing" title="Compound-term processing">Compound-term processing</a></li>
<li><a href="/wiki/Lemmatisation" class="mw-redirect" title="Lemmatisation">Lemmatisation</a></li>
<li><a href="/wiki/Lexical_analysis" title="Lexical analysis">Lexical analysis</a></li>
<li><a href="/wiki/Shallow_parsing" title="Shallow parsing">Text chunking</a></li>
<li><a href="/wiki/Stemming" title="Stemming">Stemming</a></li>
<li><a href="/wiki/Sentence_boundary_disambiguation" title="Sentence boundary disambiguation">Sentence segmentation</a></li>
<li><a href="/wiki/Word#Word_boundaries" title="Word">Word segmentation</a></li></ul>
</div></td></tr></tbody></table><div>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Automatic_summarization" title="Automatic summarization">Automatic summarization</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Multi-document_summarization" title="Multi-document summarization">Multi-document summarization</a></li>
<li><a href="/wiki/Sentence_extraction" title="Sentence extraction">Sentence extraction</a></li>
<li><a href="/wiki/Text_simplification" title="Text simplification">Text simplification</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Machine_translation" title="Machine translation">Machine translation</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Computer-assisted_translation" title="Computer-assisted translation">Computer-assisted</a></li>
<li><a href="/wiki/Example-based_machine_translation" title="Example-based machine translation">Example-based</a></li>
<li><a href="/wiki/Rule-based_machine_translation" title="Rule-based machine translation">Rule-based</a></li>
<li><a href="/wiki/Statistical_machine_translation" title="Statistical machine translation">Statistical</a></li>
<li><a href="/wiki/Transfer-based_machine_translation" title="Transfer-based machine translation">Transfer-based</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">Neural</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Distributional_semantics" title="Distributional semantics">Distributional semantics</a> models</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/Document-term_matrix" title="Document-term matrix">Document-term matrix</a></li>
<li><a href="/wiki/Explicit_semantic_analysis" title="Explicit semantic analysis">Explicit semantic analysis</a></li>
<li><a href="/wiki/FastText" title="FastText">fastText</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">large</a></li>
<li><a href="/wiki/Small_language_model" title="Small language model">small</a></li></ul></li>
<li><a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">Latent semantic analysis</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a></li>
<li><a class="mw-selflink selflink">Word embedding</a></li>
<li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Language_resource" title="Language resource">Language resources</a>,<br />datasets and corpora</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Types and<br />standards</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Corpus_linguistics" title="Corpus linguistics">Corpus linguistics</a></li>
<li><a href="/wiki/Lexical_resource" title="Lexical resource">Lexical resource</a></li>
<li><a href="/wiki/Linguistic_Linked_Open_Data" title="Linguistic Linked Open Data">Linguistic Linked Open Data</a></li>
<li><a href="/wiki/Machine-readable_dictionary" title="Machine-readable dictionary">Machine-readable dictionary</a></li>
<li><a href="/wiki/Parallel_text" title="Parallel text">Parallel text</a></li>
<li><a href="/wiki/PropBank" title="PropBank">PropBank</a></li>
<li><a href="/wiki/Semantic_network" title="Semantic network">Semantic network</a></li>
<li><a href="/wiki/Simple_Knowledge_Organization_System" title="Simple Knowledge Organization System">Simple Knowledge Organization System</a></li>
<li><a href="/wiki/Speech_corpus" title="Speech corpus">Speech corpus</a></li>
<li><a href="/wiki/Text_corpus" title="Text corpus">Text corpus</a></li>
<li><a href="/wiki/Thesaurus_(information_retrieval)" title="Thesaurus (information retrieval)">Thesaurus (information retrieval)</a></li>
<li><a href="/wiki/Treebank" title="Treebank">Treebank</a></li>
<li><a href="/wiki/Universal_Dependencies" title="Universal Dependencies">Universal Dependencies</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Data</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/BabelNet" title="BabelNet">BabelNet</a></li>
<li><a href="/wiki/Bank_of_English" title="Bank of English">Bank of English</a></li>
<li><a href="/wiki/DBpedia" title="DBpedia">DBpedia</a></li>
<li><a href="/wiki/FrameNet" title="FrameNet">FrameNet</a></li>
<li><a href="/wiki/Google_Ngram_Viewer" class="mw-redirect" title="Google Ngram Viewer">Google Ngram Viewer</a></li>
<li><a href="/wiki/UBY" title="UBY">UBY</a></li>
<li><a href="/wiki/WordNet" title="WordNet">WordNet</a></li>
<li><a href="/wiki/Wikidata" title="Wikidata">Wikidata</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Automatic_identification_and_data_capture" title="Automatic identification and data capture">Automatic identification<br />and data capture</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="/wiki/Speech_segmentation" title="Speech segmentation">Speech segmentation</a></li>
<li><a href="/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>
<li><a href="/wiki/Natural_language_generation" title="Natural language generation">Natural language generation</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">Optical character recognition</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Topic_model" title="Topic model">Topic model</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Document_classification" title="Document classification">Document classification</a></li>
<li><a href="/wiki/Latent_Dirichlet_allocation" title="Latent Dirichlet allocation">Latent Dirichlet allocation</a></li>
<li><a href="/wiki/Pachinko_allocation" title="Pachinko allocation">Pachinko allocation</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Computer-assisted_reviewing" title="Computer-assisted reviewing">Computer-assisted<br />reviewing</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Automated_essay_scoring" title="Automated essay scoring">Automated essay scoring</a></li>
<li><a href="/wiki/Concordancer" title="Concordancer">Concordancer</a></li>
<li><a href="/wiki/Grammar_checker" title="Grammar checker">Grammar checker</a></li>
<li><a href="/wiki/Predictive_text" title="Predictive text">Predictive text</a></li>
<li><a href="/wiki/Pronunciation_assessment" title="Pronunciation assessment">Pronunciation assessment</a></li>
<li><a href="/wiki/Spell_checker" title="Spell checker">Spell checker</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Natural-language_user_interface" title="Natural-language user interface">Natural language<br />user interface</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Chatbot" title="Chatbot">Chatbot</a></li>
<li><a href="/wiki/Interactive_fiction" title="Interactive fiction">Interactive fiction</a></li>
<li><a href="/wiki/Question_answering" title="Question answering">Question answering</a></li>
<li><a href="/wiki/Virtual_assistant" title="Virtual assistant">Virtual assistant</a></li>
<li><a href="/wiki/Voice_user_interface" title="Voice user interface">Voice user interface</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Related</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Formal_semantics_(natural_language)" title="Formal semantics (natural language)">Formal semantics</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Natural_Language_Toolkit" title="Natural Language Toolkit">Natural Language Toolkit</a></li>
<li><a href="/wiki/SpaCy" title="SpaCy">spaCy</a></li></ul>
</div></td></tr></tbody></table></div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374" /><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1314944253" /></div><div role="navigation" class="navbox" aria-labelledby="Artificial&#95;intelligence&#95;(AI)7556" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374" /><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231" /><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence_navbox" title="Template:Artificial intelligence navbox"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence_navbox" title="Template talk:Artificial intelligence navbox"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence_navbox" title="Special:EditPage/Template:Artificial intelligence navbox"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Artificial&#95;intelligence&#95;(AI)7556" style="font-size:114%;margin:0 4em"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a> (AI)</div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a>
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">timeline</a></li></ul></li>
<li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_companies" title="List of artificial intelligence companies">Companies</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Parameter" title="Parameter">Parameter</a>
<ul><li><a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">Hyperparameter</a></li></ul></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Double_descent" title="Double descent">Double descent</a></li>
<li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton method</a></li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient method</a></li></ul></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Normalization_(machine_learning)" title="Normalization (machine learning)">Normalization</a>
<ul><li><a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a></li></ul></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" class="mw-redirect" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Gating_mechanism" title="Gating mechanism">Gating</a></li>
<li><a href="/wiki/Weight_initialization" title="Weight initialization">Weight initialization</a></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_data_sets" title="Training, validation, and test data sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Prompt_engineering" title="Prompt engineering">Prompt engineering</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a>
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Imitation_learning" title="Imitation learning">Imitation</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Latent_diffusion_model" title="Latent diffusion model">Latent diffusion model</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Retrieval-augmented_generation" title="Retrieval-augmented generation">RAG</a></li>
<li><a href="/wiki/Uncanny_valley" title="Uncanny valley">Uncanny valley</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reflection_(artificial_intelligence)" class="mw-redirect" title="Reflection (artificial intelligence)">Reflection</a></li>
<li><a href="/wiki/Recursive_self-improvement" title="Recursive self-improvement">Recursive self-improvement</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a class="mw-selflink selflink">Word embedding</a></li>
<li><a href="/wiki/Vibe_coding" title="Vibe coding">Vibe coding</a></li>
<li><a href="/wiki/AI_safety" title="AI safety">Safety</a> (<a href="/wiki/AI_alignment" title="AI alignment">Alignment</a>)</li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Applications</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/Prompt_engineering#In-context_learning" title="Prompt engineering">In-context learning</a></li></ul></li>
<li><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Reasoning_model" title="Reasoning model">Reasoning</a></li></ul></li>
<li><a href="/wiki/Model_Context_Protocol" title="Model Context Protocol">Model Context Protocol</a></li>
<li><a href="/wiki/Intelligent_agent" title="Intelligent agent">Intelligent agent</a></li>
<li><a href="/wiki/Artificial_human_companion" title="Artificial human companion">Artificial human companion</a></li>
<li><a href="/wiki/Humanity%27s_Last_Exam" title="Humanity&#39;s Last Exam">Humanity's Last Exam</a></li>
<li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence (AGI)</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a>
<ul><li><a href="/wiki/15.ai" title="15.ai">15.ai</a></li>
<li><a href="/wiki/ElevenLabs" title="ElevenLabs">ElevenLabs</a></li></ul></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a>
<ul><li><a href="/wiki/Whisper_(speech_recognition_system)" title="Whisper (speech recognition system)">Whisper</a></li></ul></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/Text-to-image_model" title="Text-to-image model">Text-to-image models</a>
<ul><li><a href="/wiki/Aurora_(text-to-image_model)" class="mw-redirect" title="Aurora (text-to-image model)">Aurora</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Adobe_Firefly" title="Adobe Firefly">Firefly</a></li>
<li><a href="/wiki/Flux_(text-to-image_model)" title="Flux (text-to-image model)">Flux</a></li>
<li><a href="/wiki/Ideogram_(text-to-image_model)" title="Ideogram (text-to-image model)">Ideogram</a></li>
<li><a href="/wiki/Imagen_(text-to-image_model)" title="Imagen (text-to-image model)">Imagen</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Recraft" title="Recraft">Recraft</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul></li>
<li><a href="/wiki/Text-to-video_model" title="Text-to-video model">Text-to-video models</a>
<ul><li><a href="/wiki/Dream_Machine_(text-to-video_model)" title="Dream Machine (text-to-video model)">Dream Machine</a></li>
<li><a href="/wiki/Runway_(company)#Services_and_technologies" title="Runway (company)">Runway Gen</a></li>
<li><a href="/wiki/MiniMax_(company)#Hailuo_AI" title="MiniMax (company)">Hailuo AI</a></li>
<li><a href="/wiki/Kling_AI_(company)" class="mw-redirect" title="Kling AI (company)">Kling</a></li>
<li><a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a></li>
<li><a href="/wiki/Veo_(text-to-video_model)" title="Veo (text-to-video model)">Veo</a></li></ul></li>
<li><a href="/wiki/Music_and_artificial_intelligence" title="Music and artificial intelligence">Music generation</a>
<ul><li><a href="/wiki/Riffusion" title="Riffusion">Riffusion</a></li>
<li><a href="/wiki/Suno_AI" class="mw-redirect" title="Suno AI">Suno AI</a></li>
<li><a href="/wiki/Udio" title="Udio">Udio</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Text</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/T5_(language_model)" title="T5 (language model)">T5</a></li>
<li><a href="/wiki/Llama_(language_model)" title="Llama (language model)">Llama</a></li>
<li><a href="/wiki/Chinchilla_(language_model)" title="Chinchilla (language model)">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">GPT</a>
<ul><li><a href="/wiki/GPT-1" title="GPT-1">1</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">3</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">J</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">4</a></li>
<li><a href="/wiki/GPT-4o" title="GPT-4o">4o</a></li>
<li><a href="/wiki/OpenAI_o1" title="OpenAI o1">o1</a></li>
<li><a href="/wiki/OpenAI_o3" title="OpenAI o3">o3</a></li>
<li><a href="/wiki/GPT-4.5" title="GPT-4.5">4.5</a></li>
<li><a href="/wiki/GPT-4.1" title="GPT-4.1">4.1</a></li>
<li><a href="/wiki/OpenAI_o4-mini" title="OpenAI o4-mini">o4-mini</a></li>
<li><a href="/wiki/GPT-5" title="GPT-5">5</a></li>
<li><a href="/wiki/GPT-5.1" title="GPT-5.1">5.1</a></li></ul></li>
<li><a href="/wiki/Claude_(language_model)" title="Claude (language model)">Claude</a></li>
<li><a href="/wiki/Gemini_(chatbot)" class="mw-redirect" title="Gemini (chatbot)">Gemini</a>
<ul><li><a href="/wiki/Gemini_(language_model)" title="Gemini (language model)">Gemini (language model)</a></li>
<li><a href="/wiki/Gemma_(language_model)" title="Gemma (language model)">Gemma</a></li></ul></li>
<li><a href="/wiki/Grok_(chatbot)" title="Grok (chatbot)">Grok</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/DBRX" title="DBRX">DBRX</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/IBM_Watsonx" title="IBM Watsonx">IBM Watsonx</a></li>
<li><a href="/wiki/IBM_Granite" title="IBM Granite">Granite</a></li>
<li><a href="/wiki/Huawei_PanGu" title="Huawei PanGu">PanGu-Σ</a></li>
<li><a href="/wiki/DeepSeek_(chatbot)" title="DeepSeek (chatbot)">DeepSeek</a></li>
<li><a href="/wiki/Qwen" title="Qwen">Qwen</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/AutoGPT" title="AutoGPT">AutoGPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a></li>
<li><a href="/wiki/Warren_Sturgis_McCulloch" title="Warren Sturgis McCulloch">Warren Sturgis McCulloch</a></li>
<li><a href="/wiki/Walter_Pitts" title="Walter Pitts">Walter Pitts</a></li>
<li><a href="/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a></li>
<li><a href="/wiki/Christopher_D._Manning" title="Christopher D. Manning">Christopher D. Manning</a></li>
<li><a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a></li>
<li><a href="/wiki/Shun%27ichi_Amari" title="Shun&#39;ichi Amari">Shun'ichi Amari</a></li>
<li><a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima">Kunihiko Fukushima</a></li>
<li><a href="/wiki/Takeo_Kanade" title="Takeo Kanade">Takeo Kanade</a></li>
<li><a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a></li>
<li><a href="/wiki/John_McCarthy_(computer_scientist)" title="John McCarthy (computer scientist)">John McCarthy</a></li>
<li><a href="/wiki/Nathaniel_Rochester_(computer_scientist)" title="Nathaniel Rochester (computer scientist)">Nathaniel Rochester</a></li>
<li><a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a></li>
<li><a href="/wiki/Cliff_Shaw" title="Cliff Shaw">Cliff Shaw</a></li>
<li><a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a></li>
<li><a href="/wiki/Oliver_Selfridge" title="Oliver Selfridge">Oliver Selfridge</a></li>
<li><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a></li>
<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Bernard Widrow</a></li>
<li><a href="/wiki/Joseph_Weizenbaum" title="Joseph Weizenbaum">Joseph Weizenbaum</a></li>
<li><a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a></li>
<li><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Seppo Linnainmaa</a></li>
<li><a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/John_Hopfield" title="John Hopfield">John Hopfield</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Lotfi_A._Zadeh" title="Lotfi A. Zadeh">Lotfi A. Zadeh</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/James_Goodnight" title="James Goodnight">James Goodnight</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Alex_Krizhevsky" title="Alex Krizhevsky">Alex Krizhevsky</a></li>
<li><a href="/wiki/Ilya_Sutskever" title="Ilya Sutskever">Ilya Sutskever</a></li>
<li><a href="/wiki/Oriol_Vinyals" title="Oriol Vinyals">Oriol Vinyals</a></li>
<li><a href="/wiki/Quoc_V._Le" title="Quoc V. Le">Quoc V. Le</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Andrej_Karpathy" title="Andrej Karpathy">Andrej Karpathy</a></li>
<li><a href="/wiki/Ashish_Vaswani" title="Ashish Vaswani">Ashish Vaswani</a></li>
<li><a href="/wiki/Noam_Shazeer" title="Noam Shazeer">Noam Shazeer</a></li>
<li><a href="/wiki/Aidan_Gomez" title="Aidan Gomez">Aidan Gomez</a></li>
<li><a href="/wiki/John_Schulman" title="John Schulman">John Schulman</a></li>
<li><a href="/wiki/Mustafa_Suleyman" title="Mustafa Suleyman">Mustafa Suleyman</a></li>
<li><a href="/wiki/Jan_Leike" title="Jan Leike">Jan Leike</a></li>
<li><a href="/wiki/Daniel_Kokotajlo_(researcher)" title="Daniel Kokotajlo (researcher)">Daniel Kokotajlo</a></li>
<li><a href="/wiki/Fran%C3%A7ois_Chollet" title="François Chollet">François Chollet</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision transformer (ViT)</a></li></ul></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network (CNN)</a></li>
<li><a href="/wiki/Residual_neural_network" title="Residual neural network">Residual neural network (RNN)</a></li>
<li><a href="/wiki/Highway_network" title="Highway network">Highway network</a></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network (GNN)</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/20px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/40px-Symbol_category_class.svg.png 1.5x" data-file-width="180" data-file-height="185" /></span></span> <a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Category</a></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐7d87c67667‐znc6l
Cached time: 20251127094318
Cache expiry: 51415
Reduced expiry: true
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.727 seconds
Real time usage: 0.856 seconds
Preprocessor visited node count: 3871/1000000
Revision size: 29783/2097152 bytes
Post‐expand include size: 210847/2097152 bytes
Template argument size: 3570/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 225727/5000000 bytes
Lua time usage: 0.472/10.000 seconds
Lua memory usage: 6553796/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  702.207      1 -total
 58.25%  409.001      1 Template:Reflist
 19.92%  139.884     13 Template:Cite_book
 14.96%  105.084     18 Template:Cite_journal
 13.79%   96.822      1 Template:Machine_learning_bar
 13.28%   93.247      1 Template:Sidebar_with_collapsible_lists
  9.10%   63.919      1 Template:Short_description
  7.70%   54.062      5 Template:Navbox
  6.42%   45.077      7 Template:Cite_conference
  6.15%   43.181      7 Template:Cite_arXiv
-->

<!-- Saved in parser cache with key enwiki:pcache:43561218:|#|:idhash:canonical and timestamp 20251127094318 and revision id 1323019878. Rendering was triggered because: page_view
 -->
</div><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&amp;type=1x1&amp;usesul3=1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Word_embedding&amp;oldid=1323019878">https://en.wikipedia.org/w/index.php?title=Word_embedding&amp;oldid=1323019878</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Language_modeling" title="Category:Language modeling">Language modeling</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Natural_language_processing" title="Category:Natural language processing">Natural language processing</a></li><li><a href="/wiki/Category:Computational_linguistics" title="Category:Computational linguistics">Computational linguistics</a></li><li><a href="/wiki/Category:Semantic_relations" title="Category:Semantic relations">Semantic relations</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_publisher_location" title="Category:CS1 maint: publisher location">CS1 maint: publisher location</a></li><li><a href="/wiki/Category:CS1_errors:_ISBN_date" title="Category:CS1 errors: ISBN date">CS1 errors: ISBN date</a></li><li><a href="/wiki/Category:CS1:_long_volume_value" title="Category:CS1: long volume value">CS1: long volume value</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:All_articles_lacking_reliable_references" title="Category:All articles lacking reliable references">All articles lacking reliable references</a></li><li><a href="/wiki/Category:Articles_lacking_reliable_references_from_May_2024" title="Category:Articles lacking reliable references from May 2024">Articles lacking reliable references from May 2024</a></li><li><a href="/wiki/Category:All_articles_containing_circular_references" title="Category:All articles containing circular references">All articles containing circular references</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 19 November 2025, at 04:44<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a href="/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a>;
additional terms may apply. By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use" class="extiw" title="foundation:Special:MyLanguage/Policy:Terms of Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy" class="extiw" title="foundation:Special:MyLanguage/Policy:Privacy policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a rel="nofollow" class="external text" href="https://wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.wikipedia.org/w/index.php?title=Word_embedding&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://www.wikimedia.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/static/images/footer/wikimedia-button.svg" width="84" height="29"><img src="/static/images/footer/wikimedia.svg" width="25" height="25" alt="Wikimedia Foundation" lang="en" loading="lazy"></picture></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/w/resources/assets/poweredby_mediawiki.svg" width="88" height="31"><img src="/w/resources/assets/mediawiki_compact.svg" alt="Powered by MediaWiki" lang="en" width="25" height="25" loading="lazy"></picture></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-header-container vector-sticky-header-container no-font-mode-scale">
	<div id="vector-sticky-header" class="vector-sticky-header">
		<div class="vector-sticky-header-start">
			<div class="vector-sticky-header-icon-start vector-button-flush-left vector-button-flush-right" aria-hidden="true">
				<button class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle" tabindex="-1" data-event-name="ui.vector-sticky-search-form.icon"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
			</button>
		</div>
			
		<div role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box">
			<div class="vector-typeahead-search-container">
				<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail">
					<form action="/w/index.php" id="vector-sticky-search-form" class="cdx-search-input cdx-search-input--has-end-button">
						<div  class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
							<div class="cdx-text-input cdx-text-input--has-start-icon">
								<input
									class="cdx-text-input__input mw-searchInput" autocomplete="off"
									
									type="search" name="search" placeholder="Search Wikipedia">
								<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
							</div>
							<input type="hidden" name="title" value="Special:Search">
						</div>
						<button class="cdx-button cdx-search-input__end-button">Search</button>
					</form>
				</div>
			</div>
		</div>
		<div class="vector-sticky-header-context-bar">
				<nav aria-label="Contents" class="vector-toc-landmark">
						
					<div id="vector-sticky-header-toc" class="vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left"  >
						<input type="checkbox" id="vector-sticky-header-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-sticky-header-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
						<label id="vector-sticky-header-toc-label" for="vector-sticky-header-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
						</label>
						<div class="vector-dropdown-content">
					
						<div id="vector-sticky-header-toc-unpinned-container" class="vector-unpinned-container">
						</div>
					
						</div>
					</div>
			</nav>
				<div class="vector-sticky-header-context-bar-primary" aria-hidden="true" ><span class="mw-page-title-main">Word embedding</span></div>
			</div>
		</div>
		<div class="vector-sticky-header-end" aria-hidden="true">
			<div class="vector-sticky-header-icons">
				<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-talk-sticky-header" tabindex="-1" data-event-name="talk-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-subject-sticky-header" tabindex="-1" data-event-name="subject-sticky-header"><span class="vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-history-sticky-header" tabindex="-1" data-event-name="history-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink" id="ca-watchstar-sticky-header" tabindex="-1" data-event-name="watch-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-edit-sticky-header" tabindex="-1" data-event-name="wikitext-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-ve-edit-sticky-header" tabindex="-1" data-event-name="ve-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-viewsource-sticky-header" tabindex="-1" data-event-name="ve-edit-protected-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock"></span>

<span></span>
			</a>
		</div>
			<div class="vector-sticky-header-buttons">
				<button class="cdx-button cdx-button--weight-quiet mw-interlanguage-selector" id="p-lang-btn-sticky-header" tabindex="-1" data-event-name="ui.dropdown-p-lang-btn-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language"></span>

<span>23 languages</span>
			</button>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive" id="ca-addsection-sticky-header" tabindex="-1" data-event-name="addsection-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive"></span>

<span>Add topic</span>
			</a>
		</div>
			<div class="vector-sticky-header-icon-end">
				<div class="vector-user-links">
				</div>
			</div>
		</div>
	</div>
</div>
<div class="mw-portlet mw-portlet-dock-bottom emptyPortlet" id="p-dock-bottom">
	<ul>
		
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.eqiad.main-657dc4489f-pw9zq","wgBackendResponseTime":118,"wgPageParseReport":{"limitreport":{"cputime":"0.727","walltime":"0.856","ppvisitednodes":{"value":3871,"limit":1000000},"revisionsize":{"value":29783,"limit":2097152},"postexpandincludesize":{"value":210847,"limit":2097152},"templateargumentsize":{"value":3570,"limit":2097152},"expansiondepth":{"value":16,"limit":100},"expensivefunctioncount":{"value":3,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":225727,"limit":5000000},"entityaccesscount":{"value":0,"limit":500},"timingprofile":["100.00%  702.207      1 -total"," 58.25%  409.001      1 Template:Reflist"," 19.92%  139.884     13 Template:Cite_book"," 14.96%  105.084     18 Template:Cite_journal"," 13.79%   96.822      1 Template:Machine_learning_bar"," 13.28%   93.247      1 Template:Sidebar_with_collapsible_lists","  9.10%   63.919      1 Template:Short_description","  7.70%   54.062      5 Template:Navbox","  6.42%   45.077      7 Template:Cite_conference","  6.15%   43.181      7 Template:Cite_arXiv"]},"scribunto":{"limitreport-timeusage":{"value":"0.472","limit":"10.000"},"limitreport-memusage":{"value":6553796,"limit":52428800}},"cachereport":{"origin":"mw-web.eqiad.main-7d87c67667-znc6l","timestamp":"20251127094318","ttl":51415,"transientcontent":true}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Word embedding","url":"https:\/\/en.wikipedia.org\/wiki\/Word_embedding","sameAs":"http:\/\/www.wikidata.org\/entity\/Q18395344","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q18395344","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2014-08-14T10:44:37Z","dateModified":"2025-11-19T04:44:42Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Word_embedding_illustration.svg","headline":"method in natural language processing"}</script>
</body>
</html>